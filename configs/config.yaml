# configs/config.yaml

# --- General Experiment Settings ---
simulation_name: "RansomLog_SmolLM135M_LoRA_R8"
dataset_name: "ransomlog"  # 'ransomlog' or 'hdfs'
description: "First experiment with RansomLog dataset using SmolLM-135M and LoRA rank 8."

# --- Paths ---
# All paths are relative to the project root
data_base_path: "./data/"
results_path: "./results/"

# --- Data Processing Settings ---
# These settings are specific to the dataset being used
# For RansomLog, you might define session_type: 'time_window' or 'process_id'
# and window_size: '5m'
session_window_spliter: " ;-; "
force_reprocess_data: False # If True, will re-run the data processing steps

# --- Model Settings ---
model_name: "HuggingFaceTB/SmolLM-135M"
lora: True
lora_rank: 16  # Increased from 8 for higher capacity
lora_alpha_multiplier: 4  # Increased from 2 for better stability
lora_dropout: 0.05  # Reduced from 0.1 to prevent underfitting

# --- Federated Learning Settings ---
use_parallel_training: true # Set to false to run sequentially (e.g., with CUDA_VISIBLE_DEVICES=1)
num_rounds: 50
num_clients: 50
client_frac: 0.1 # Fraction of clients to select each round

# --- Client Training Settings ---
max_steps: 50  # Increased from 10 for better learning
batch_size: 16  # Reduced from 32 for better generalization
gradient_accumulation_steps: 4  # Effective batch size of 64
initial_lr: 0.0001  # Reduced from 0.001 for more stable training
min_lr: 0.00001
lr_scheduler_type: 'cosine' # or 'constant'

# --- Evaluation Settings ---
# The k values for top-k accuracy calculation
top_k_values: [1, 3, 5, 10]
# The number of thresholds to test when finding the best F1 score
f1_threshold_steps: 1000
