# configs/config_hdfs.yaml

# --- General Experiment Settings ---
simulation_name: "HDFS_Validation_SmolLM135M_LoRA_R8"
dataset_name: "hdfs"  # This is the key change
description: "Validation experiment to replicate original project results with the HDFS dataset."

# --- Paths ---
data_base_path: "./data/"
results_path: "./results/"

# --- Data Processing Settings ---
session_window_spliter: " ;-; "
force_reprocess_data: True

# --- Model Settings ---
model_name: "HuggingFaceTB/SmolLM-135M"
lora: True
lora_rank: 8
lora_alpha_multiplier: 2
lora_dropout: 0.1

# --- Federated Learning Settings ---
use_parallel_training: true # Set to false to run sequentially (e.g., with CUDA_VISIBLE_DEVICES=1)
num_rounds: 50
num_clients: 50
client_frac: 0.1

# Data distribution and client behavior
data_distribution_strategy: "iid"
non_iid_alpha: 0.5

# Client selection and aggregation
client_selection_strategy: "uniform"
use_weighted_aggregation: false

# FedProx regularization (reduces client drift in Non-IID scenarios)
# Set fedprox_mu > 0 to enable. Recommended values: 0.001, 0.01, 0.1
# Reference: https://arxiv.org/abs/1812.06127
fedprox_mu: 0.0  # Disabled for IID scenario

# --- Client Training Settings ---
max_steps: 10
batch_size: 8
gradient_accumulation_steps: 4  # effective batch = 8 * 4 = 32 (same as legacy)
initial_lr: 0.001
min_lr: 0.00001
lr_scheduler_type: 'cosine'
trainer_lr_scheduler_type: 'constant'  # Legacy behavior: LR calculated externally, Trainer uses constant
weight_decay: 0.01
fp16: true
optim: 'paged_adamw_8bit'

# --- Legacy Compatibility (for reproducing original HDFS results) ---
use_legacy_tokenization: true  # Adds padding_side="right" in tokenization
use_legacy_trainer: true       # Don't use data_collator (like original utils.py)

# --- Evaluation Settings ---
top_k_values: [1, 3, 5, 10]
f1_threshold_steps: 1000
evaluator_version: "old"  # Legacy: balanced 1000/1000 evaluation
