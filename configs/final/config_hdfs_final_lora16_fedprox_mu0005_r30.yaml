# HDFS version of: LoRA16 + FedProx(mu=0.0005) no regime OrigLike

simulation_name: "HDFS_FINAL_LoRA16_FedProx_0005_Orig_R30"
dataset_name: "hdfs"
description: "HDFS Final: FedProx mu=0.0005, LoRA16, LR=3e-4, frac=0.2, rounds=30 (original scoring)"

data_base_path: "./data/"
results_path: "./results/"

session_window_spliter: " ;-; "
force_reprocess_data: false

model_name: "HuggingFaceTB/SmolLM-135M"
lora: True
lora_rank: 16
lora_alpha_multiplier: 4
lora_dropout: 0.1

use_parallel_training: false
num_rounds: 30
num_clients: 50
client_frac: 0.2

data_distribution_strategy: "iid"
non_iid_alpha: 0.5

client_selection_strategy: "uniform"
use_weighted_aggregation: false

fedprox_mu: 0.0005

max_steps: 50
batch_size: 4
gradient_accumulation_steps: 16
initial_lr: 0.0003
min_lr: 0.00001
lr_scheduler_type: "cosine"
trainer_lr_scheduler_type: "constant"  # Legacy behavior: LR calculated externally, Trainer uses constant
weight_decay: 0.01
fp16: true
optim: 'paged_adamw_8bit'

top_k_values: [1, 3, 5, 10]
f1_threshold_steps: 1000
enable_temporal_metrics: false

threshold_selection: "f1_max"
evaluator_version: "old"  # Legacy: balanced 1000/1000 evaluation
accuracy_method: "original"
