# HDFS run aligned with the original legacy README/scripts, but reproducible.
#
# Run:
#   ./.venv/bin/python main.py --config configs/legacy/config_hdfs_readme_legacy.yaml

simulation_name: "HDFS_LEGACY_README_SmolLM135M_LoRA_R8"
dataset_name: "hdfs"
description: "Legacy/README-compatible HDFS run (reproducible, legacy split/selection/layout + legacy evaluator)."

# --- Paths ---
data_base_path: "./data/"
# Match legacy folder naming (./fl-results/<sim>/round_*/...)
results_path: "./fl-results/"

# --- Legacy toggles (opt-in; keep other experiments unchanged) ---
legacy_seed: 0
legacy_hdfs_shuffle: true
legacy_keep_text_column: true
legacy_client_split: true
legacy_client_eval_split: 0.1
legacy_flresults_layout: true
legacy_client_selection_systemrandom: true
legacy_4bit_training: true
legacy_training_args: true

# --- Data Processing Settings ---
session_window_spliter: " ;-; "
force_reprocess_data: true

# --- Model Settings ---
model_name: "HuggingFaceTB/SmolLM-135M"
lora: true
lora_rank: 8
lora_alpha_multiplier: 2
lora_dropout: 0.1
# Stabilize LoRA targets across peft versions.
lora_target_modules: ["q_proj", "v_proj"]

# --- Federated Learning Settings ---
use_parallel_training: false
num_rounds: 50
num_clients: 50
client_frac: 0.1

data_distribution_strategy: "iid"
client_selection_strategy: "uniform"
use_weighted_aggregation: false
fedprox_mu: 0.0

# --- Client Training Settings (README: effective batch=32 via accumulation) ---
max_steps: 10
batch_size: 8
gradient_accumulation_steps: 4
initial_lr: 0.001
min_lr: 0.00001
lr_scheduler_type: "cosine"
trainer_lr_scheduler_type: "constant"
weight_decay: 0.01
fp16: true
optim: "paged_adamw_8bit"

# --- Legacy Compatibility ---
use_legacy_tokenization: true
use_legacy_trainer: true

# --- Evaluation Settings ---
top_k_values: [1, 3, 5, 10]
f1_threshold_steps: 1000
evaluator_version: "old"
