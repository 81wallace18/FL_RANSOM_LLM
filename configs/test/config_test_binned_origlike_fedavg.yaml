# configs/test/config_test_binned_origlike_fedavg.yaml
# Test config: enables the new binned flow-to-text representation + training stabilizers.
#
# Notes:
# - Set `force_reprocess_data: true` so train/test/tokenized are regenerated with the new `Content`.
# - If `lora_target_modules` errors (module names differ), remove that field to fall back to PEFT defaults.

# --- General Experiment Settings ---
simulation_name: "TEST_Binned_OrigLike_FedAvg_R10"
dataset_name: "edge_ransomware"
description: "Orig-like smoke test with binned features (FedAvg, 10 rounds)"

# --- Paths ---
data_base_path: "./data/ids_ransomware/"
results_path: "./results/"

# --- Data Processing Settings ---
session_window_spliter: " ;-; "
force_reprocess_data: true
benign_train_fraction: 0.8

# Use the new binned representation (quantile bins computed on benign train only).
content_mode: "binned"          # "raw" (default) or "binned"
binning_num_bins: 32
binning_transform: "log1p"      # "log1p" or "none"

# --- Model Settings ---
model_name: "HuggingFaceTB/SmolLM-135M"
lora: True
lora_rank: 32
lora_alpha_multiplier: 4
lora_dropout: 0.1

# Optional: fix LoRA injection points (comment out if it fails for this model)
lora_target_modules: "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"

# --- Federated Learning Settings ---
use_parallel_training: true
num_rounds: 10
num_clients: 50
client_frac: 0.2

data_distribution_strategy: "hetero_device"
non_iid_alpha: 0.5

client_selection_strategy: "data_size_proportional"
use_weighted_aggregation: true

fedprox_mu: 0.0

# --- Client Training Settings ---
max_steps: 50
batch_size: 4
gradient_accumulation_steps: 16
initial_lr: 0.0003
min_lr: 0.00001
lr_scheduler_type: 'cosine'

# New stabilizers
warmup_ratio: 0.05
max_grad_norm: 1.0

# --- Evaluation Settings ---
top_k_values: [1, 3, 5, 10]
f1_threshold_steps: 1000
enable_temporal_metrics: false

threshold_selection: "f1_max"

evaluator_version: "new"
accuracy_method: "original"

