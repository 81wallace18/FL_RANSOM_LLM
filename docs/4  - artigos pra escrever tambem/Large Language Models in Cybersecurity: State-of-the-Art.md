## Large Language Models in Cybersecurity: State-of-the-Art

Farzad Nourmohammadzadeh Motlagh

Hasso-Plattner-Institute for Digital Engineering, University of Potsdam Germany farzad.motlagh@hpi.de

## Pejman Najafi

Hasso-Plattner-Institute for Digital Engineering, University of Potsdam Germany pejman.najafi@hpi.de

## ABSTRACT

The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.

## KEYWORDS

LLM, Large Language Model, AI, cybersecurity, advanced threats, cyberattacks, cyberdefense, privacy and security.

## 1 INTRODUCTION

The evolution of generative artificial intelligence, notably large language models (LLMs), has influenced most disciplines of science and technology that support content generation in diverse applications [60]. In education, LLMs support educators in various tasks such as assignment assessment [36], question generation [22], providing feedback [30], and essay grading [92]. In the entertainment industry, LLMs demonstrate competitive performance in generating music captions [19] as well as video game scripts [51]. Automation is introduced into customer service [63], marketing [27, 94], and supply chain management [34, 47, 52] through the integration of LLMs in business. Meanwhile, the utilization of LLMs in healthcare enables professionals by providing real-time clinical decision support [24, 69], medical education [50, 81], and prediction of disease progression [70, 78].

With advancements in cyber threats, the cybersecurity domain can also be equipped with cutting-edge tools, assisting cybersecurity practitioners who continuously seek solutions to implement advanced policies or strengthen technological protections against the disclosure of confidential information, unauthorized access, and other forms of data modification [43]. Thanks to LLMs' capability in

## Mehrdad Hajizadeh

Technische Universitat Chemnitz Germany mehrdad.hajizadeh@etit.tu-

chemnitz.de

## Feng Cheng

Hasso-Plattner-Institute for Digital Engineering, University of Potsdam Germany feng.cheng@hpi.de

Mehryar Majd

Hasso-Plattner-Institute for Digital Engineering, University of Potsdam Germany mehryar.majd@hpi.de

## Christoph Meinel

Hasso-Plattner-Institute for Digital Engineering, University of Potsdam Germany christoph.meinel@hpi.de breaking down complex natural language patterns, security experts are now enabled to explore more attack vectors in various contexts associated with textual data [93].

Functionalities of LLMs are increasingly being integrated into the cybersecurity posture, contributing to promising enhancements in cybersecurity defense applications [53]. Through analyzing vast amounts of text data, including security logs, these models can identify emerging vulnerabilities. Anomaly detection represents a key application of LLMs for identifying potential threats [55]. Furthermore, LLMs mitigate potential risks by offering automated vulnerability fixes, aiming to improve organizations' security posture [64].

However, with the continuous advancements of LLMs in cyber defense, it is crucial to acknowledge that these language models can also be leveraged by malicious actors. For example, LLMs can be misused by attackers to execute malware in target companies [8], engage in defense evasion [12], and gain access to credentials [68]. The potential to generate complex and personalized phishing messages further highlights the misuse of LLMs for deceiving people in an organization, paving the way for unauthorized access to companies' sensitive information [72]. To further elaborate, WormGPT [23] is an AI-powered tool designed for cybercriminals to automate the generation of personalized phishing emails. Although it may sound somewhat similar to ChatGPT, WormGPT is not a friendly neighborhood AI; instead, its purpose is to produce malicious content. Furthermore, FraudGPT [21] enabled attacker to create content to convince users to click on a particular generated link.

Thedual nature of LLMs has transformed the cybersecurity realm by offering new challenges and opportunities. Developing robust defensive strategies to foresee attacks and address concerns related to the utilization of LLMs motivated us to formulate a taxonomy of strategies appearing in the field of cybersecurity. To define our contributions more precisely, this paper addresses:

- The intersection of LLMs' offensive approaches as a newly introduced dimension to cybersecurity is framed in this study in line with the Mitre attack framework [14].
- Exploring LLM-empowered defensive strategies in dealing with potential threats and malware based on the NIST cybersecurity framework [15].

- Understanding the major functionalities of LLMs in current research trends alongside potential applications in the cybersecurity landscape.

The rest of paper is organized as follows: In Section 2, we provide an overview of LLMs . Moving forward to Section 3, we explore cyber threat defenses leveregred by LLMs where Section 4 outlines sophisticated attacks designed by LLMs. Finally, Section 5 concludes the challenges posed by LLMs in the context of cybersecurity.

## 2 BACKGROUND

LLMs are neural networks that learn from textual data to process various language-related tasks [59]. From Eliza as a pattern recognition chatbot in the 1960s [89], over the years several advancements pushed Natural Language Processing (NLP) forward, such as long short-term memories to handle a wide range of data [35], Stanford CoreNLP suite [56] providing a collection of algorithms to perform intricate NLP tasks and continued with transformer architecture [86].

A breakthrough in Transformer-based models surged the field of NLP and led to the development of numerous kinds of effective LLMs. T5 [66] applied language modeling in pre-trained LLMs, where spans are altered with a single mask. GPT-3 enhanced the performance of LLMs with size by increasing model parameters to 175B. PaLM-2 is trained on high-quality datasets [5] with an objective of cutting the cost of training and inference [59]. Llama, a set of decoder-only models aimed at minimizing the amount of activations in the backward step [59, 84]. Xuan Yuan 2.0, a Chinese financial chat model [59, 97], AlexaTM [80], PaLM-2 [5], as well as GLM-130B [96] are a few instances of general purpose pre-trained LLMs. While pre-trained models offer an essential understanding of languages, as AI advances, fine-tuning LLMs boost business functions and satisfaction by fulfilling industry-specific criteria [99]. A general-purpose LLaMA-GPT-4 [65], Goat [54] for handling complicated arithmetic queries, HuaTuo [88] a medical knowledge model, Evol-Instruct [91] offering complicated prompts, and LLaMA 2-Chat fine-tuned using rejection sampling [84] are exemplary instruction-tuning LLMs. Running in higher costs, extensive hardware requirements, cost of slow training on various tasks, limited LLMs utilization [59]. Retrieving support evidence from an external in-domain knowledge base [98], parameter tuning and knowledge distillation are among the techniques extensively researched for effective LLM deployment [59].

Recently, the scientific literature has experienced a significant growth in the number of articles related to LLMs, principally driven by their proven efficacy across a wide range of functions. As a result, throughout various surveys, researchers attempted to categorize these advancements in LLM architecture [37, 59, 100, 101]. Though previous studies have investigated literature reviews to highlight the safety aspects of LLMs [1, 38, 39, 49], the present study focuses primarily on the application of LLMs in the context of cyberdefense as well as cyberattack.

## 3 DEFENSIVE APPLICATIONS OF LLMS

In the field of cybersecurity, the National Institute of Standards and Technology (NIST) provides a comprehensive structure to enhance organizations' cybersecurity status, as detailed in the NIST

cybersecurity framework [15]. According to its effectiveness and popularity in cyberdefense, we classify the diverse array of LLMcentered approaches that contributed in cyberdefense through the lens of NIST framework to better understand the impact of LLMs in cyberdefense. The framework consists of a structured approach to identify, protect, detect, respond to, and recover from cybersecurity threats and incidents.

## 3.1 Identify

The process of developing an organizational understanding to manage cybersecurity risk concerning systems, assets, data, and capabilities is referred to the Identify function in the context of the NIST framework [15]. Identifying potential risks is a crucial phase in risk management, and LLMs aim to fulfill a transformational role in forming risk management in businesses. Johnson [41] presents invaluable insights for policymakers on the applicability of LLMs to risk management. According to the author, LLMs go through business headlines, social media posts, economic indicators, legal documentation, and other key sources, emphasizing risk elements to deliver more accurate and predictive risk assessments that a human analyst might overlook. Lima et al. [17] develop a risk matrix from application reviews using LLMs. Through user feedback, they proposed an automatic prompt extraction technique. These prompts were passed into LLMs, which classified the risks into five classes ranging from negligible to critical for further investigation. Naleszkiewicz [58] discusses LLM applications allowing companies to overcome traditional enterprise risk management challenges, such as operational and compliance risks. LLMs evaluate unstructured siloed data across various departments, acting as a bridge to provide an in-depth understanding of an organization's risk profile. Furthermore, LLMs boost risk modeling by generating expert opinions based on prior patterns, risk mitigation by generating contingency plans, and risk reporting by providing customized risk assessments.

## 3.2 Protect

Implementing safeguards to guarantee the delivery of essential services is reflected in protect function [15]. It involves various mechanisms such as maintaining a proactive security posture or prioritizing cybersecurity awareness and training to empower the organization's workforce. In the current digital environment, proactive protection technologies are essential since they enable companies to anticipate and prevent troubles before they arise. For example, proactive technologies empower enterprises to minimize the likelihood of coming across inappropriate content, and thus reduce the possibility of experiencing ethical or legal challenges [82]. Voros et al. [87] harnessed the power of LLMs to enhance web content filtration. They have improved the accuracy of web content categorization by scanning of large amount of URLs. Another research accomplished by Yu et al. [95] investigates GPT-3's capacity to produce honeywords to trap the attackers if they are using deceptive generated passwords. First, they extract the components of the original password using a password-specific segmentation algorithm. These segments are then fed into GPT-3 as a prompt to generate a collection of passwords similar to the input password.

A crucial element in this model's efficacy is the maintenance of strong password components called chunks given to the LLM [76].

LLMs can play a valuable role in strengthening cybersecurity awareness and training within the protect function of the NIST framework. Tann et al. [83] apply LLMs to tackle professional certification topics and perform Capture The Flag (CTF) tasks to improve participants' cybersecurity education. LLMs have significance by enabling attendees to explore CTF test settings, providing explanations to concerns connected to professional certification, and highlighting the need to model cybersecurity breach scenarios in CTF sessions to support the development of more comprehensive skills. However, LLMs face limitations when it comes to responding to conceptual queries. Furthermore, LLMs can improve team collaboration by offering security question solutions that are suitable for inexperienced as well as experts. For instance, LLMs greatly increase the efficacy of penetration test teams by making it easier for team members to pass on information by offering more in-depth assessments and generating appropriate explanations to be on the same page about the detected risks. Moreover, LLMs serve as a connection between experts and publicly accessible web resources, in particular assisting specialists in remaining up to date on the most recent security concerns that are critical to their company [20].

Automated vulnerability fixing with LLMs diminishes the risk of cyberattacks. A three-step process is described by Charalambous et al. [10] for addressing automotive vulnerability issues. Bounded Model Checking (BMC) is the first step in the process. It evaluates the user-provided source code to a property specification. The original code and the appropriate counterexample are provided to the LLM module by the BMC engine in the scenario that this phase's verification is unsuccessful and a security property violation is detected. Secondly, customized queries are sent to the LLM engine to produce a corrected version of the code. Lastly, the BMC module re-evaluates the code that the LLM module changed to formally determine whether the updated version matches the original security and safety requirements.

Automating flaw mitigation can be facilitated by LLMs if the defect is well-defined and the prompt provides additional information. While these models were fully effective in fixing simulated vulnerabilities, real-world scenarios presented challenges for their performance. The primary challenges stem from the numerous methods that information is presented, the complexities of prompt processing and code development in LLMs, and the significance of prompt phrasing, which can result in notable variations in the code required to be generated [64]. Furthermore, Sandoval et al. [75] performed an examination of potentially insecure code suggestions during the process of code development. Within a particular programming context that the authors had defined, they tested scenarios with and without AI support. Their findings indicate that users assisted by AI develop security flaws at a rate lower than ten percent, suggesting that using LLMs in their security-oriented research does not present major new security risks.

## 3.3 Detect

The NIST framework's Detect function serves to identify cybersecurity events as they arise [15]. Exploring anomaly detection in system logs is a crucial step toward developing effective detection methods through the use of LLMs. Recurrent Neural Network Language Models are used by Tuor et al. [85] to present an unsupervised, online anomaly detection method for computer security log analysis. This approach simplifies the usual effort-intensive feature engineering stage, making it fast to implement, and is independent of the tools used for system configuration and monitoring. The authors have demonstrated the efficacy of their approach by utilizing the Los Alamos National Laboratory Cyber Security Dataset [44]. Their findings indicate that the approach can be handled in real-time, generating and organizing log-line-level anomaly scores while taking into account inter-log-line context. The authors [85] considered metrics including Average Percentile (AP) and Area under the Receiver Operator Characteristic Curve (AUC) to show how the false-positive rate dropped without significantly affecting the ability to detect unusual behavior [44].

GPT-2 is used by VulDetect[61], a transformer-based vulnerability detection framework, to detect anomalies in system logs. Using a dataset containing both vulnerable and non-vulnerable code, the model is fine-tuned to detect anomalies that represent regular behavior. Malicious behavior is defined as any unexpected or unlikely outcome that the model possibly generated. Two benchmark datasets, SARD [102] and SeVC [77], were utilized by the authors to assess VulDetect's performance. The outcomes showed that VulDetect has a low false positive rate and is efficient in realtime vulnerability detection. Moreover, the integration of LLMs into penetration testing practices has the potential to revolutionize the world of threat detection. Threat detection could undergo a revolution if LLMs are incorporated into penetration testing procedures. Happe et al.'s investigation [31] focused on using LLMs to improve penetration testing. In line with their classification, LLMs provide advancement in two aspects of penetration testing: high-level and low-level operations. High-level assignments include conceptual investigation and strategic planning, such as finding out about emerging active directory attacks. On the other hand, tasks at a lower level incorporate consideration of practical activities involving system exploitation and vulnerability analysis. This entails looking for specific attack vectors for a particular system.

A further investigation by Deng et al. [18] introduces PENTESTGPT, an automated penetration testing system driven by LLMs. Complex tasks such as question answering, summarization, and reasoning are readily handled with PENTESTGPT. Addressing context loss concerns and simulating human behavior in penetration testing are the objectives. Three self-interacting modules jointly form PENTESTGPT including reasoning, generation, and parsing. These modules collaborate to tackle penetration testing problems by using a divide-and-conquer approach. Specific subtasks are allocated to each module, which interact to effectively handle and compile the data generated during testing.

Ranade et al. [67] improve the processing of threats, attacks, and vulnerabilities which is challenging due to the high volume of data, and the dynamic nature of evolving attack techniques. The primary objective of their research is an enhanced version of a BERT model, which aims to effectively perform several cybersecurity-related operations. Using Masked Language Modeling (MLM), the model was trained using unstructured and semi-structured open-source Cyber Threat Intelligence (CTI) data. Its evaluation encompassed diverse

downstream tasks with potential applications in Security Operations Centers (SOCs). They additionally offer real-world examples of how to apply CyBERT to cybersecurity problems. Several subsequent works have furthered the advancements of this research in terms of both training efficiency and accuracy such as SecureBERT [2], CySecBERT [6], and ClaimsBERT [4]. In this regard, Bayer et al. [6] presented a word embedding model based on BERT and collected a dataset from multiple sources. This adaptation makes the model capable of coping with a wide range of cybersecurity tasks, namely malware detection, alert aggregation, and phishing website detection.

The LLMs can also facilitate auditory tasks to detect vulnerabilities among the smart contracts. David et al. [16] utilized LLMs to target vulnerabilities in the smart contracts and DeFi protocol layers. Their study detects 52 compromised DeFi protocols, as input data for the language model context, evaluating the impact of model temperature and context length on the language model's efficacy in smart contract auditing. The results indicated that incorporating LLMs into the audit workflow substantially boost the effectiveness and accuracy of analyzing an array of feasible attacks. On the other hand, Chen et al. [13] trained LLM on a dataset of 10,000 smart contracts and evaluated how well it detected nine different vulnerabilities. According to the authors' findings, LLMs frequently deliver false positive results when detecting smart contract vulnerabilities. This might be connected with interference from incomplete codes or LLMs' incapacity to understand code segments.

An LLM can be used to build a scenario comparable to an attacker's strategy for gaining access to an organization's property by exploiting a vulnerability. Garvey et al. [29] study the viability of using Generative-AI to improve the development of Red Team scenarios in organizations. The authors [29] propose employing LLMs to construct narratives based on prompts or questions as input. Subsequently, subject-matter specialists provide remarks, including modifying narratives, adding new elements, or integrating multiple items to develop more complex scenarios. The objective is to guarantee that the generated scenarios are plausible and adhere to the provided framework. They found that including elements inspired by fiction into LLMs improves creativity and imagination in the scenario development process.

Koide et al. [46] present a strategy for detecting phishing websites using LLMs. Their approach entails using a web crawler to retrieve data from websites and creating prompts for LLMs. Social engineering strategies are then identified by evaluating the context of entire web pages and URLs. The prompts rely on the Chain of Thought (CoT) prompting technique, which enables LLMs to elaborate on their reasoning. In addition, the study recommends an HTML simplification approach to improve efficiency. This entails lowering the token count by simplifying HTML text and removing HTML elements that lack text within tags, such as style, script, and comment tags. This operation is repeated until the token count reaches a certain threshold, thus boosting overall efficiency.

Sakaoglu introduced KARTAL[73], a fine-tuned Language Model for detecting vulnerabilities in web applications. A detector component in the KARTAL system is controlled by the prompts from the prompter component. These prompts are generated based on input gathered by the fuzzer component, which monitors application activity. The LLM detects logical vulnerabilities in web applications, specifically broken access control rules, by analyzing these prompts. This technique allows KARTAL to dynamically alter the definitions of broken access, allowing it to adapt to a variety of scenarios. This adaptability distinguishes it from less intelligent vulnerability scanners, allowing KARTAL to be more effective in its detection capabilities.

LLMsdemonstrate their capacity to be an effective method across a wide range of vulnerability identification tasks. CyBERT [3] unveils a classifier for detecting cybersecurity feature claims. The method incorporates fine-tuning a pre-trained BERT language model to recognize cybersecurity claims throughout complex sequences observed in industrial control systems (ICS) device documentation. This is accomplished by aggregating reports for each feature from every source linked with an individual device, effectively determining in-conflict feature claims. The extraction of sequences from ICS-related documents is the initial stage in the procedure as these sequences are classified into broad claims, device claims, or cybersecurity claims. Then, the identified sequences are used to train CyBERT so it can classify new sequences.

SecurityLLM, a system developed for precise threat detection and data privacy, is presented by Ferrag et al. [26]. SecurityLLM utilizes Fixed-Length Language Encoding (FLLE) as a privacy-preserving encoding method, in conjunction with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer forming text traffic data. The SecurityLLM framework is composed of two primary components: SecurityBERT, which detects cyber threats, and FalconLLM, which responds to and recovers from incidents. The method, which was trained on an IoT cybersecurity dataset, displays significant accuracy in identifying fourteen various types of cyber threats.

SecureFalcon [25] is an LLM-based cybersecurity reasoning system targeted to detect software flaws. The method involves finetuning FalconLLM with the use of a FormAI dataset including C code instances. SecureFalcon [25] uses binary classification to distinguish between vulnerable and non-vulnerable patterns and then validates corrected code using Bounded Model Checking. However, the study's adaptability is limited due to the FormAI dataset's exclusive focus on C codes.

## 3.4 Respond

The Respond function involves the formulation of actions to address the detected incident [15]. The convergence of LLMs and honeypot paradigms enhances the capability to respond to malware threats. In exploring this synergy, McKee et al. [57] research the feasibility of using LLMs to improve cybersecurity in a honeypot setup. The researchers [57] demonstrate how these chatbots can create a responsive honeypot interface capable of responding to illicit activities. This method gives security professionals more time to respond to an ongoing cyber attack. Ten tasks connected with the development of honeypots are divided into three primary categories by the authors [57]: networks, operating systems, and applications. Their results indicate that the LLM-based honeypot interfaces are able to maintain the attacker's interest over the course of several inquiries. In another study, Sladic et al. [79] present an LLM-based technique for developing software honeypots. The devised honeypot named shelLM is designed to evaluate the credibility of the model through

Figure 1: The present bar chart illustrates the distribution of studies mapped to each of the five elements of the NIST Cybersecurity Framework. Collected statistics indicate that the vast amount of studies are related to Protect and Detect functions emphasizing research gaps related to Identify, Respond and particularly Recover functions over collected publications.

<!-- image -->

the use of security experts in an experiment. The specialists collaborated with ShelLM to assess how it responded to the commands of an attacker. ShelLM's ability to retain consistency over several sessions is a significant feature; the content of each terminal session is kept and used as a prompt for following sessions. This makes sure that regardless of when a session comes to an end interactions can carry on without interruption. Cambiaso et al. [9] deliver a method for generating email messages to identified attackers in order to engage them and squander their resources. LLMs provide realistic responses based on human behavior, making scams less profitable. However, such automated responses need a significant amount of storage and computational power.

We provide a set of insights based on existing work in Table 1. Thepresent pattern of published papers on the use of LLMs for cyber defense indicates that most studies are focused on the detection and protection roles of LLMs aligning with the NIST framework. However, a research gap, as shown in Figure 1, becomes evident in post-attack scenarios. Given the critical roles recovery and attack response play in the cybersecurity lifecycle, it is essential that further studies be centered around the development of innovative LLM-related solutions to maximize their potential in productive post-attack scenarios.

## 4 ADDVERSARIAL APPLICATION OF LLMS

Applications of LLMs in cybersecurity extend beyond techniques for defense. In our exploration, we review LLMs' capacity to come up with sophisticated attacks. To this end, our approach involves with analyzing these approaches through the MITRE attack framework, which outlines various attacker tactics.

## 4.1 Reconnaissance

During a reconnaissance attack, adversaries actively or passively collect information about their target organization in order to identify upcoming operations [90]. Hazell [32] provides an illustration of howLLMsassist during the reconnaissance stage by automating the data collection and analysis of potential victims. As a result, LLMs develop Python scripts to scrape websites that hold the desired information about users. Comparably, Salewski et al. [74] enabled the LLMs to assume various roles by introducing the prompt with "If you were a persona", in which the target individual is substituted for the persona.

## 4.2 Initial Access

The initial access tactic includes the procedures adopted by attackers to obtain access as a foothold to a company's infrastructure [90]. Roy et al. [72] highlight the role of LLMs in delivering malicious scripts where the attack structure is divided into four steps. In this regard, design objects are used to create concepts that are influenced by specific organizations, while credential-stealing objects are used to establish objects that require credentials, including login buttons or input fields. Credential Transfer objects are used to create functions that can provide the attacker with the credentials submitted on phishing websites. Lastly, the exploit generation object serves to implement a functionality based on the evasive exploit. The authors [72] conduct a number of attacks, including text encoding, clickjacking, polymorphic URL, and QR code-based multi-stage attacks, to show how LLMs have the potential to be leveraged to generate a variety of phishing attack forms.

According to Hazell et al. [32], LLMs are able to assist during the reconnaissance stage of a spear phishing attack, a process when attackers get sensitive information about their targets in order to develop compelling messages. According to John et al. [40], MLbased techniques group people according to their value and level of participation, and then utilize the timeliness of the target users to provide content and a phishing URL. Since people can adopt different personas in daily life and choose a variety of terms for a variety of circumstances, Kreps et al. [48] discuss how GPT2 can manipulate target users' beliefs by generating stories, while Salewski et al. [74] investigate the role of LLMs on various personas and adapt their language accordingly a process known as in-context impersonation. Based on LLMs ability to impersonate certain personalities, Salewski et al. [74] concluded that LLMs can be applied to develop more effective phishing messages or social engineering attacks. With a dataset of phishing emails, Karanjai [42] investigates the effectiveness of generating convincing phishing emails with GPT2, GPT-3, and LSTM while taking into account the removal of HTML elements, URLs, and email addresses as well as tokenizing the text into words.

PassGPT, an LLM-based approach to password generation and modeling for password estimation, is presented by Rando et al. [68]. PassGPT presents the idea of guided password generation, enabling the generation of passwords that adhere to established standards. Moreover, PassGPT, trained on password leaks, models each token independently, a character-by-character search space exploration in which generated passwords are sampled according to random restrictions.

The application of LLMs, particularly ChatGPT and AutoGPT, in malware generation is covered by Pa Pa et al. [62]. To determine if Auto-GPT minimizes the obstacle to malware generation, the authors [62] investigated Auto-GPT running locally and tested it in the following manners: initially, by providing broad prompts like

Table 1: Classified publications concerning the defensive applications of LLMs.

| Paper   |   Year | NIST Framework   | Application                                                                                                  | Model(s)                                                                                     |
|---------|--------|------------------|--------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| [45]    |   2023 | Identify         | LLMs enhance cybersecurity policies.                                                                         | ChatGPT                                                                                      |
| [33]    |   2023 | Protect          | Using LLMs for secure code development without compromising functionality.                                   | SVEN (GPT-2), (CodeGen) LM                                                                   |
| [83]    |   2023 | Protect          | LLMs solve Capture The Flag challenges to enhance employees' awareness and knowledge.                        | code-cushman-001, code-davinci-001,code-davinci-002, 1-jumbo, j1-large, polycoder, gpt2-csrc |
| [64]    |   2023 | Protect          | LLMs investigate software vulnerabilities.                                                                   | GPT-3.5 Turbo, Gemini, Microsoft Bing                                                        |
| [10]    |   2023 | Protect          | LLMs investigate software vulnerabilities.                                                                   | GPT-3.5 Turbo                                                                                |
| [95]    |   2023 | Protect          | Generating honeywords using LLMs.                                                                            | GPT-3                                                                                        |
| [20]    |   2018 | Protect          | Chatbots assist security experts in identifying open ports.                                                  | Rule-based                                                                                   |
| [87]    |   2023 | Protect          | LLM-based URL categorization for website classification.                                                     | eXpose (Conv), BERTiny, URLTran (BERT) T5 Large, GPT3 Babbage                                |
| [75]    |   2023 | Protect          | LLMs investigate code vulnerabilities.                                                                       | GPT-3                                                                                        |
| [85]    |   2018 | Detect           | Detecting anomalous behavior in network logs with LLMs.                                                      | RNN                                                                                          |
| [61]    |   2023 | Detect           | Detection of vulnerabilities in software code.                                                               | GPT-2                                                                                        |
| [28]    |   2023 | Detect           | SecureBERT for anomaly detection.                                                                            | CyBERT, SecureBERT (RoBERTa)                                                                 |
| [67]    |   2021 | Detect           | CyBERT, a domain-specific BERT model to recognize specialized cybersecurity entities.                        | BERT-based Natural Language Filter                                                           |
| [31]    |   2023 | Detect           | Penetration testing with LLMs.                                                                               | GPT-3.5                                                                                      |
| [3]     |   2021 | Detect           | CyBERT, a cybersecurity feature claims classifier.                                                           | CyBERT, GPT-2                                                                                |
| [6]     |   2022 | Detect           | CySecBERT for malware detection and alert aggregation.                                                       | CySecBERT                                                                                    |
| [6]     |   2022 | Detect           | SecureBERT for processing and understandin cybersecurity text, specifically Cyber Threat Intelligence (CTI). | SecureBERT                                                                                   |
| [25]    |   2023 | Detect           | Detection of vulnerabilities in software code.                                                               | SecureFalcon (FalconLLM)                                                                     |
| [79]    |   2023 | Respond          | Creating honeypots related to continuously monitoring and detecting threats.                                 | GPT-3.5 Turbo (shelLM)                                                                       |
| [57]    |   2023 | Respond          | LLM as a honeypot interface against command-line attacks.                                                    | GPT-3.5                                                                                      |
| [29]    |   2023 | Detect           | investigates LLMs acting as red teamers in cybersecurity.                                                    | GPT-4 & Bard                                                                                 |
| [46]    |   2023 | Detect           | LLM for detecting phishing sites leverages a web crawler to gather information and generate prompts.         | GPT-3.5 & GPT-4                                                                              |
| [73]    |   2023 | Detect           | KARTAL, a web application vulnerability detection.                                                           | GPT-3.5 Turbo                                                                                |
| [16]    |   2023 | Detect           | LLMs to perform security audits on smart contracts.                                                          | GPT-4 (GPT-4-32k), Claude-v1.3-100k                                                          |
| [18]    |   2023 | Detect           | LLM-empowered automatic penetration testing tool.                                                            | PentestGPT (GPT-3.5 & GPT-4)                                                                 |
| [13]    |   2023 | Detect           | LLMs to perform security audits on smart contracts.                                                          | GPT-3.5 Turbo & GPT-4                                                                        |
| [9]     |   2023 | Respond          | Replying to the scam emails using LLM.                                                                       | GPT-3                                                                                        |

"write a malware X," and next, by giving more specific malware and attack tool functionalities. Finally, additional tests have been explored to discover whether Anti-Virus (AV), Endpoint Detection and Response (EDR), and VirusTotal (VT) detect the generated malware.

## 4.3 Execution

Procedures resulting in adversary-controlled executable operating on a local or remote system are referred to as execution [90]. Using code generation tools to develop malware is one of the strategies employed by adversaries. The feasibility of employing large textual models to automatically generate malware along with the model's constraints when generating actual malware samples is studied by Botacin [8]. According to their findings, certain malware versions were recognized by all antivirus engines while others were not detected by any of the engines due to the use of LLMs to modify all or part of the malware's building blocks. The prompt engineering essential to develop malware that hides a PowerShell and schedules its daily execution at a given time was brought to light by Charan et al. [11]. In addition to copying the CMD file to a designated directory and getting the scheduled task information as a successful malware verification, the script adds a registry value that will be run at system startup. The LLM-based malware is assessed by Pa pa et al. [62]. The authors [62] reported that a number of the commercially available antivirus applications and Endpoint Detection and Response (EDR) solutions failed to detect the LLM-generated executables since some LLM-generated functions can establish connections toward attackers through the victim's machine [7].

## 4.4 Defense Evasion

The concept of defense evasion outlines the tactics attackers employ in order to prevent detection following a security breach [90]. According to Chatzoglou et al. [12], LLMs develop turnkey malware which lets adversaries evade antivirus and endpoint detection and response systems aiming to autonomous malicious code development. Process injection, multiprocessing, junk data, shellcode mem loading, encryption, and chosen shell code were among the techniques employed in their investigation. According to Chatzoglou et al. [12] LLMs establish an initial TCP listener that resembles an SSH listener. This will let an attacker to connect and use Windows native APIs to execute Command Prompt (cmd) instructions. An open firewall port is required for the listener to function properly.

Table 2: Classified publications concerning the adversarial applications of LLMs.

| Paper   |   Year | MITRE Tactic(s)                              | Application                                                | Model(s)                               |
|---------|--------|----------------------------------------------|------------------------------------------------------------|----------------------------------------|
| [11]    |   2023 | Execution                                    | Generating code to perform actions that could be malicious | GPT-3                                  |
| [42]    |   2022 | Initial Access                               | Generate phishing emails to bypass spam filters            | GPT-2, GPT-3, RoBERTa                  |
| [7]     |   2022 | Execution - Command & Control                | Use of LLMs as plug-ins to act as a proxy                  | GPT-4                                  |
| [72]    |   2023 | Initial Access - Collection                  | Generate Phishing Website via ChatGBT                      | GPT-3.5 Turbo                          |
| [8]     |   2023 | Execution                                    | Code generation and DLL injection                          | GPT-3                                  |
| [32]    |   2023 | Initial Access - Reconnaissance              | Collecting victim data to develop an attack email          | GPT-3.5, GPT-4                         |
| [62]    |   2023 | Initial Access - Execution - Defense Evasion | Crafting malicious scripts                                 | GPT-3.5 Turbo, GPT-4, text-davinci-003 |
| [40]    |   2018 | Initial Access                               | Spear Phishing link                                        | AWD-LSTM                               |
| [12]    |   2023 | Defense Evasion                              | Code obfuscation, file format modification                 | GPT-3.5                                |
| [68]    |   2023 | Initial Access - Credential Access           | Password guessing using LLMs                               | GPT-2                                  |
| [74]    |   2023 | Initial Access - Reconnaissance              | Impersonation for phishing aims                            | GPT-3.5 Turbo                          |
| [48]    |   2022 | Initial Access                               | Generating content for misinformation                      | GPT-2                                  |

Figure 2: Concentration of recently published papers on attack approaches using LLM

<!-- image -->

Only three of the twelve antivirus applications were able to identify malware, according to the author's findings [12].

The study conducted by Pa Pa et al. [62] assesses the effectiveness of malware scanners in detecting both obfuscated and nonobfuscated forms of code generated by LLMs. In contrast to LLMbased commonly used obfuscation techniques including base64 encoding or variable and function name modification, the authors [62] demonstrated that generated non-obfuscated malware featured a reduced detection rate.

The use of evasive approaches by LLMs to evade detection by anti-phishing organizations is highlighted by Roy et al. [71]. This study illustrates how LLMs assist attackers via clickjacking, fingerprinting browsers, or encoding content. Accordingly, the content of the phishing website is masked using these tactics, making it more challenging for automated anti-phishing crawlers to identify malicious information.

## 4.5 Credential Access

Approaches to get credentials through key-logging or credential dumping from a compromised machine refer to credential access [90]. Introduced by Rando et al. [68], PassGPT is an LLM-based password modeling solution. PassGPT uses GPT-2 architecture to estimate password strength and guess passwords. Additionally, the authors [68] analyze the probability distribution through passwords defined by PassGPT. In light of this, PassGPT delivers guided password generation, enabling constraints to choose character level randomization for the search space by setting parameters like password length or fixed characters with complete control over each character.

## 4.6 Collection

Collection refers to gathering information related to the attackers goals [90]. Methodologies that demonstrate how LLMs assist in gathering user data are covered by Roy et al. [71]. The authors [71] investigate the applicability of LLMs in the design of credential taking objects with generating input forms. Furthermore, LLMs have the capability to distribute iFrame injection code to launch malicious websites within an official page. Roy et al. [71] demonstrate a scam attack implemented via ChatGPT to gather information without direct attempt aimed at automated data collection. The presented scam item has a hidden iFrame associated with a malicious as well as fake Amazon webpage, guaranteeing that the iFrame object does not activate any anti cross site scripting.

## 4.7 Command and Control

Attacks known as command and control arise when an attacker uses a victim channel to connect with underlying resources [90]. By leveraging LLMs for performing shell commands on a victim's resource, Beckerich et al. [7] demonstrate the notion of a command and control attack. In order to generate the executable and automate connection between the machine used by the victim and servers, the authors utilized an LLM-based plugin that acts as an interface for communicating with GPT-2. This method involves utilizing a connectivity feature to establish a connection to a certain website that hosts an attacker's command, followed by a query that ends in a URL. A list of valid user agents used by plugins is maintained regularly in order to mask the malicious component of the web server.

Figure 2 depicts the study trends on the use of LLMs in cyberattacks, and Table 2 provides a summary of the categorization. Figure 2 illustrates that initial access, defense evasion, and execution tactics are the primary points of concentration for the majority of attack methodologies. As a result, cybersecurity professionals must to give priority to these crucial phases while developing strategic protection methods against LLM-based attacks.

## 5 CONCLUSION

In this paper, we reviewed the state-of-the-art research in the applications of Large Language Models (LLMs) within the realm of cybersecurity. We demonstrated that while LLMs can provide effective solutions for strengthening defensive approaches, their potential misuse cannot be underestimated. Hence, we categorized related literature using the NIST cybersecurity framework and MITRE attack for applications of LLMs in cyberdefense and cyberattacks, respectively. Our review suggests that while there are numerous works evaluating the opportunities in defensive applications of LLMs, there is a lack of research in examining the risks of offensive applications. We hope this study paves the way for future research to assess the associated risks introduced by the rise of LLMs in cybersecurity.

## REFERENCES

- [1] Samuel Addington. 2023. ChatGPT: Cyber Security Threats and Countermeasures. Available at SSRN 4425678 (2023).
- [2] Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. 2022. SecureBERT: A Domain-Specific Language Model for Cybersecurity. In International Conference on Security and Privacy in Communication Systems . Springer, 39-56.
- [3] Kimia Ameri, Michael Hempel, Hamid Sharif, Juan Lopez Jr, and Kalyan Perumalla. 2021. Cybert: Cybersecurity claim classification by fine-tuning the bert language model. Journal of Cybersecurity and Privacy 1, 4 (2021), 615-637.
- [4] Kimia Ameri, Michael Hempel, Hamid Sharif, Juan Lopez Jr, and Kalyan Perumalla. 2022. An accuracy-maximization approach for claims classifiers in document content analytics for cybersecurity. Journal of Cybersecurity and Privacy 2, 2 (2022), 418-443.
- [5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).
- [6] Markus Bayer, Philipp Kuehn, Ramin Shanehsaz, and Christian Reuter. 2022. CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain. arXiv preprint arXiv:2212.02974 (2022).
- [7] Mika Beckerich, Laura Plein, and Sergio Coronado. 2023. RatGPT: Turning online LLMs into Proxies for Malware Attacks. arXiv preprint arXiv:2308.09183 (2023).
- [8] Marcus Botacin. 2023. Gpthreats-3: Is automatic malware generation a threat?. In 2023 IEEE Security and Privacy Workshops (SPW) . IEEE, 238-254.
- [9] Enrico Cambiaso and Luca Caviglione. 2023. Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources. arXiv preprint arXiv:2303.13521 (2023).
- [10] Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, and Lucas C Cordeiro. 2023. A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification. arXiv preprint arXiv:2305.14752 (2023).
- [11] PVCharan, Hrushikesh Chunduri, P Mohan Anand, and Sandeep K Shukla. 2023. FromText to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads. arXiv preprint arXiv:2305.15336 (2023).
- [12] Efstratios Chatzoglou, Georgios Karopoulos, Georgios Kambourakis, and Zisis Tsiatsikas. 2023. Bypassing antivirus detection: old-school malware, new tricks. arXiv preprint arXiv:2305.04149 (2023).
- [13] Chong Chen, Jianzhong Su, Jiachi Chen, Yanlin Wang, Tingting Bi, Yanli Wang, Xingwei Lin, Ting Chen, and Zibin Zheng. 2023. When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We? arXiv preprint arXiv:2309.05520 (2023).
- [14] MITRE Corporation. 2023. MITRE ATTACK. https://attack.mitre.org/matrices/ enterprise/
- [15] Critical Infrastructure Cybersecurity. 2014. Framework for improving critical infrastructure cybersecurity. Framework 1, 11 (2014).
- [16] Isaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, and Arthur Gervais. 2023. Do you still need a manual smart contract audit? arXiv preprint arXiv:2306.12338 (2023).
- [17] Vitor Mesaque Alves de Lima, Jacson Rodrigues Barbosa, and Ricardo Marcondes Marcacini. 2023. Learning Risk Factors from App Reviews: A Large Language Model Approach for Risk Matrix Construction. (2023).
- [18] Gelei Deng, Yi Liu, Vctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, and Stefan Rass. 2023. Pentestgpt: An llm-empowered automatic penetration testing tool. arXiv preprint arXiv:2308.06782 (2023).
- [19] Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, and Emmanouil Benetos. 2023. MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. arXiv preprint arXiv:2309.08730 (2023).
- [20] Saurabh Dutta, Ger Joyce, and Jay Brewer. 2018. Utilizing chatbots to increase the efficacy of information security practitioners. In Advances in Human Factors in Cybersecurity: Proceedings of the AHFE 2017 International Conference on Human Factors in Cybersecurity, July 17- 21, 2017, The Westin Bonaventure Hotel, Los Angeles, California, USA 8 . Springer, 237-243.
- [21] Tushar Subhra Dutta. 2023. FraudGPT: New Black Hat AI Tool Launched by Cybercriminals. https://cybersecuritynews.com/fraudgpt-new-black-hat-aitool/
- [22] Sabina Elkins, Ekaterina Kochmar, Iulian Serban, and Jackie CK Cheung. 2023. How Useful are Educational Questions Generated by Large Language Models?. In International Conference on Artificial Intelligence in Education . Springer, 536542.
- [23] Polra Victor Falade. 2023. Decoding the threat landscape: Chatgpt, fraudgpt, and wormgpt in social engineering attacks. arXiv preprint arXiv:2310.05595 (2023).
- [24] Sahar Fawzi. 2023. A Review of the Role of ChatGPT for Clinical Decision Support Systems. In 2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES) . IEEE, 439-442.
- [25] Mohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Merouane Debbah, Thierry Lestable, and Lucas C Cordeiro. 2023. SecureFalcon: The Next Cyber Reasoning System for Cyber Security. arXiv preprint arXiv:2307.06616 (2023).
- [26] Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C Cordeiro, Merouane Debbah, and Thierry Lestable. 2023. Revolutionizing Cyber Threat Detection with Large Language Models. arXiv preprint arXiv:2306.14263 (2023).
- [27] Chunjing Gan, Dan Yang, Binbin Hu, Ziqi Liu, Yue Shen, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, and Guannan Zhang. 2023. Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation. arXiv preprint arXiv:2312.05276 (2023).
- [28] Mingze Gao. 2023. The Advance of GPTs and Language Model in Cyber Security. Highlights in Science, Engineering and Technology 57 (2023), 195-202.
- [29] Bruce Garvey and Adam Svendsen. 2023. Can Generative-AI (ChatGPT and Bard) Be Used as Red Team Avatars in Developing Foresight Scenarios? Analytic Research Consortium (ARC) August (2023).
- [30] Kai Guo and Deliang Wang. 2023. To resist it or to embrace it? Examining ChatGPT's potential to support teacher feedback in EFL writing. Education and Information Technologies (2023), 1-29.
- [31] Andreas Happe and Jrgen Cito. 2023. Getting pwn'd by AI: Penetration Testing with Large Language Models. arXiv preprint arXiv:2308.00121 (2023).
- [32] Julian Hazell. 2023. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972 (2023).
- [33] Jingxuan He and Martin Vechev. 2023. Large language models for code: Security hardening and adversarial testing. (2023).
- [34] Christian Hendriksen. 2023. AI for Supply Chain Management: Disruptive Innovation or Innovative Disruption? Journal of Supply Chain Management (2023).
- [35] Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780.
- [36] Ya-Ping Hsiao, Nadia Klijn, and Mei-Shiu Chiu. 2023. Developing a framework to re-design writing assignment assessment for the era of Large Language Models. Learning: Research and Practice 9, 2 (2023), 148-158.
- [37] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403 (2022).
- [38] Shotaro Ishihara. 2023. Training Data Extraction From Pre-trained Language Models: A Survey. arXiv preprint arXiv:2305.16157 (2023).
- [39] Eider Iturbe, Erkuden Rios, Angel Rego, and Nerea Toledo. 2023. Artificial Intelligence for next generation cybersecurity: The AI4CYBER framework. In Proceedings of the 18th International Conference on Availability, Reliability and Security . 1-8.
- [40] S John and T Philip. 2018. Generative models for spear phishing posts on social media. In NIPS Workshop On Machine Deception, California, USA. arXiv .
- [41] Andrew Johnson. 2023. The Transformative Role of Large Language Models in Enterprise Risk Management. https://medium.com/@andrew\_johnson\_4/thetransformative-role-of-large-language-models-in-enterprise-riskmanagement-7d46b494e73fium
- [42] Rabimba Karanjai. 2022. Targeted phishing campaigns using large scale language models. arXiv preprint arXiv:2301.00665 (2022).
- [43] Ramanpreet Kaur, Duan Gabrijeli, and Toma Klobuar. 2023. Artificial intelligence for cybersecurity: Literature review and future research directions. Information Fusion (2023), 101804.
- [44] Alexander D Kent. 2016. Cyber security data sources for dynamic network research. In Dynamic Networks and Cyber-Security . World Scientific, 37-65.
- [45] Benjamin Kereopa-Yorke. 2023. Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia. arXiv preprint arXiv:2306.02612

(2023).

- [46] Takashi Koide, Naoki Fukushi, Hiroki Nakano, and Daiki Chiba. 2023. Detecting Phishing Sites Using ChatGPT. arXiv preprint arXiv:2306.05816 (2023).
- [47] Edward Elson Kosasih, Emmanuel Papadakis, George Baryannis, and Alexandra Brintrup. 2023. A review of explainable artificial intelligence in supply chain management using neurosymbolic approaches. International Journal of Production Research (2023), 1-31.
- [48] Sarah Kreps, R Miles McCain, and Miles Brundage. 2022. All the news that's fit to fabricate: AI-generated text as a tool of media misinformation. Journal of experimental political science 9, 1 (2022), 104-117.
- [49] Andrei Kucharavy, Zachary Schillaci, Loc Marchal, Maxime Wrsch, Ljiljana Dolamic, Remi Sabonnadiere, Dimitri Percia David, Alain Mermoud, and Vincent Lenders. 2023. Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. arXiv preprint arXiv:2303.12132 (2023).
- [50] Ian J Kuckelman, H Yi Paul, Molinna Bui, Ifeanyi Onuh, Jade A Anderson, and Andrew B Ross. 2023. Assessing ai-powered patient education: a case study in radiology. Academic Radiology (2023).
- [51] Gaetan Lopez Latouche, Laurence Marcotte, and Ben Swanson. 2023. Generating video game scripts with style. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023) . 129-139.
- [52] Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. 2023. Large language models for supply chain optimization. arXiv preprint arXiv:2307.03875 (2023).
- [53] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. 2023. Privacy in large language models: Attacks, defenses and future directions. arXiv preprint arXiv:2310.10383 (2023).
- [54] Tiedong Liu and Bryan Kian Hsiang Low. 2023. Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks. arXiv preprint arXiv:2305.14201 (2023).
- [55] Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, and Xun Chen. 2023. LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis. arXiv preprint arXiv:2308.07610 (2023).
- [56] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations . 55-60.
- [57] Forrest McKee and David Noever. 2023. Chatbots in a honeypot world. arXiv preprint arXiv:2301.03771 (2023).
- [58] Kris Naleszkiewicz. 2023. Harnessing LLMs in Enterprise Risk Management: A New Frontier in Decision-making. https://ai.plainenglish.io/harnessing-largelanguage-models-llms-in-enterprise-risk-management-erm-7174df33da9b
- [59] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435 (2023).
- [60] Subash Neupane, Ivan A Fernandez, Sudip Mittal, and Shahram Rahimi. 2023. Impacts and Risk of Generative AI Technology on Cyber Defense. arXiv preprint arXiv:2306.13033 (2023).
- [61] Marwan Omar and Stavros Shiaeles. 2023. VulDetect: A novel technique for detecting software vulnerabilities using Language Models. In 2023 IEEE International Conference on Cyber Security and Resilience (CSR) . IEEE, 105-110.
- [62] Yin Minn Pa Pa, Shunsuke Tanizaki, Tetsui Kou, Michel Van Eeten, Katsunari Yoshioka, and Tsutomu Matsumoto. 2023. An attacker's dream? exploring the capabilities of chatgpt for developing malware. In Proceedings of the 16th Cyber Security Experimentation and Test Workshop . 10-18.
- [63] Keivalya Pandya and Mehfuza Holia. 2023. Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations. arXiv preprint arXiv:2310.05421 (2023).
- [64] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining zero-shot vulnerability repair with large language models. In 2023 IEEE Symposium on Security and Privacy (SP) . IEEE, 2339-2356.
- [65] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023).
- [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551.
- [67] Priyanka Ranade, Aritran Piplai, Anupam Joshi, and Tim Finin. 2021. Cybert: Contextualized embeddings for the cybersecurity domain. In 2021 IEEE International Conference on Big Data (Big Data) . IEEE, 3334-3342.
- [68] Javier Rando, Fernando Perez-Cruz, and Briland Hitaj. 2023. PassGPT: Password Modeling and (Guided) Generation with Large Language Models. arXiv preprint arXiv:2306.01545 (2023).
- [69] Arya Rao, John Kim, Meghana Kamineni, Michael Pang, Winston Lie, and Marc D Succi. 2023. Evaluating ChatGPT as an adjunct for radiologic decisionmaking. medRxiv (2023), 2023-02.
- [70] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. MedBERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. NPJ digital medicine 4, 1 (2021), 86.
- [71] Sayak Saha Roy, Krishna Vamsi Naragam, and Shirin Nilizadeh. 2023. Generating Phishing Attacks using ChatGPT. arXiv preprint arXiv:2305.05133 (2023).
- [72] Sayak Saha Roy, Krishna Vamsi Naragam, and Shirin Nilizadeh. 2023. Generating Phishing Attacks using ChatGPT. arXiv e-prints (2023), arXiv-2305.
- [73] Sinan Sakaoglu. 2023. KARTAL: Web Application Vulnerability Hunting Using Large Language Models: Novel method for detecting logical vulnerabilities in web applications with finetuned Large Language Models.
- [74] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. 2023. In-Context Impersonation Reveals Large Language Models' Strengths and Biases. arXiv preprint arXiv:2305.14930 (2023).
- [75] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt. 2023. Lost at c: A user study on the security implications of large language model code assistants. arXiv preprint arXiv:2208.09727 (2023).
- [76] Satya Sannihith Lingutla. 2023. Enhancing password security: advancements in password segmentation technique for high-quality honeywords. (2023).
- [77] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).
- [78] Ofir Ben Shoham and Nadav Rappoport. 2023. Cpllm: Clinical prediction with large language models. arXiv preprint arXiv:2309.11295 (2023).
- [79] Muris Sladi, Veronica Valeros, Carlos Catania, and Sebastian Garcia. 2023. LLM in the Shell: Generative Honeypots. arXiv preprint arXiv:2309.00155 (2023).
- [80] Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, et al. 2022. Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448 (2022).
- [81] Haifeng Song, Yi Xia, Zhichao Luo, Hui Liu, Yan Song, Xue Zeng, Tianjie Li, Guangxin Zhong, Jianxing Li, Ming Chen, et al. 2023. Evaluating the Performance of different large language models on health consultation and patient education in urolithiasis. Journal of Medical Systems 47, 1 (2023), 125.
- [82] Nan Sun, Ming Ding, Jiaojiao Jiang, Weikang Xu, Xiaoxing Mo, Yonghang Tai, and Jun Zhang. 2023. Cyber Threat Intelligence Mining for Proactive Cybersecurity Defense: A Survey and New Perspectives. IEEE Communications Surveys &amp; Tutorials (2023).
- [83] Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, and Ee-Chien Chang. 2023. Using Large Language Models for Cybersecurity Capture-TheFlag Challenges and Certification Questions. arXiv preprint arXiv:2308.10443 (2023).
- [84] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
- [85] Aaron Randall Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, and Robert Jasper. 2018. Recurrent neural network language models for open vocabulary event-level cyber anomaly detection. In Workshops at the thirty-second AAAI conference on artificial intelligence .
- [86] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
- [87] Tams Vrs, Sean Paul Bergeron, and Konstantin Berlin. 2023. Web Content Filtering through knowledge distillation of Large Language Models. arXiv preprint arXiv:2305.05027 (2023).
- [88] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975 (2023).
- [89] Joseph Weizenbaum. 1966. ELIZA-a computer program for the study of natural language communication between man and machine. Commun. ACM 9, 1 (1966), 36-45.
- [90] Wenjun Xiong, Emeline Legrand, Oscar berg, and Robert Lagerstrm. 2022. Cyber security threat modeling based on the MITRE Enterprise ATTACK Matrix. Software and Systems Modeling 21, 1 (2022), 157-177.
- [91] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).
- [92] Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, and Dragan Gaevi. 2023. Practical and ethical challenges of large language models in education: A systematic literature review. arXiv preprint arXiv:2303.13379 (2023).
- [93] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023).

- [94] Qi Yang, Marlo Ongpin, Sergey Nikolenko, Alfred Huang, and Aleksandr Farseev. 2023. Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising. In Proceedings of the 31st ACM International Conference on Multimedia . 9299-9305.
- [95] Fangyi Yu and Miguel Vargas Martin. 2023. Honey, I Chunked the Passwords: Generating Semantic Honeywords Resistant to Targeted Attacks Using Pretrained Language Models. In International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment . Springer, 89-108.
- [96] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).
- [97] Xuanyu Zhang and Qing Yang. 2023. Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management . 44354439.
- [98] Yating Zhang, Yexiang Wang, Fei Cheng, Sadao Kurohashi, et al. 2023. Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise.

arXiv preprint arXiv:2310.03328 (2023).

- [99] Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun Ma, Yingtong Bu, Xun Zhou, and Liang Zhao. 2023. Balancing specialized and general skills in llms: The impact of modern tuning and data strategy. arXiv preprint arXiv:2310.04945 (2023).
- [100] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).
- [101] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419 (2023).
- [102] Xin Zhou and Rakesh M Verma. 2022. Vulnerability detection via multimodal learning: datasets and analysis. In Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security . 1225-1227.