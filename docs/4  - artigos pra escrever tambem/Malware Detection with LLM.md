2025 Communication and Information Technologies (KIT) | 979-8-3315-7339-3/25/$31.00 ©2025 IEEE | DOI: 10.1109/KIT67756.2025.11205440

## Malware Detection with LLM

Armed Forces Academy of General Milan Rastislav Štefánik

Matej ADAMEC Simulation Centre Liptovský Mikuláš, Slovak Republic matej.adamec@aos.sk

Abstract -The  increasing  sophistication  of  malware  poses critical challenges to traditional detection techniques, particularly  in  the  face  of  polymorphic  and  evasive  threats. Recent advances in Natural Language  Processing (NLP), specifically  through  the  deployment  of  large  language  models (LLMs),  offer promising  capabilities for addressing these challenges.  This  study  evaluates  the  effectiveness  of  three LLMs-GPT-2,  T5,  and  CodeBERT-in  detecting  malware from decompiled .c code derived from Portable Executable (PE) files. The models were tested on a balanced dataset containing real-world malware and benign samples, preprocessed using a custom  tokenization  and  classification  pipeline. Experimental results indicate that GPT-2 and T5 achieved strong performance. The study confirms the viability of transformerbased LLMs for source-code-based malware detection, particularly GPT-2 and T5, and emphasizes the importance of model selection and dataset quality. Future work will focus on refining these architectures using larger, more diverse training sets and incorporating class-weighted loss functions to further improve detection balance and reduce false positives.

Keywords- LLM, Malware Detection, GPT 2, T5, CodeBERT

## I. INTRODUCTION

The rapid evolution of malware threats poses significant cyber  security  challenges.  Traditional  malware  detection techniques often struggle to keep up with sophisticated and polymorphic malware that constantly adapts to evade detection. Recent advances in Natural Language Processing (NLP),  particularly  through  the  use  of  Large  Language Models, offer a promising new approach to addressing these challenges [1].

Large Language Models have demonstrated considerable potential in a range of cybersecurity tasks, including malware detection. Primarily recognised for their capacity to process and generate  human-like text,  these  models  are  now  being deployed to detect and analyse malware due to their ability to comprehend intricate data structures and discern concealed patterns [2].

Conventional  malware  detection  techniques  frequently rely  on  signature-based  methods, heuristics, or behavioural analysis.  However,  these  approaches  frequently  encounter challenges in effectively countering novel, polymorphic, or previously  unobserved  threats.  LLMs,  however,  have  been shown to be more effective in handling these challenges by learning to recognise patterns in software behaviour, even in the  absence  of  a  pre-existing  signature.  LLMs'  ability  to understand  complex  relationships  and  contextual  nuances makes  them  an  ideal  candidate  for  improving  malware detection systems.

Michal TURČANÍK Dept. of Informatics Armed Forces Academy of General Milan Rastislav Štefánik Liptovský Mikuláš, Slovak Republic michal.turcanik@aos.sk

LLMs  are  trained  on  large datasets that include a combination  of  both  benign  and  malicious  samples.  These datasets contain various forms of data, including raw binary files, system call patterns, and code generated from decompiled executables. The objective of training an LLM is to enable it to identify malicious activity or behaviours, even from unknown or novel malware samples.

However,  selecting  the  appropriate  dataset  for  training LLMs in malware detection is a critical and challenging task. The quality and diversity of the dataset directly influence the model's performance and ability to generalise to  new malware types. Datasets with a wide range of malware types, behaviours, and obfuscation techniques are vital for training robust models. The challenge of creating such datasets stems from the sheer volume of malware, the constant evolution of attack methods, and the need to ensure the data is appropriately labelled [3, 4, 5].

The innovative approach to dataset creation that has been developed involves the use of decompiled executables, with a particular focus on those in the Portable Executable (PE) format, which is commonly utilised by Windows operating systems. Utilising tools such as Ghidra, a reverse engineering tool, enables the decompilation of PE files, resulting in their conversion  into  a  higher-level  language  like  C  code.  This transformation facilitates more efficient analysis of the code's structure  and  behaviour.  The  dataset  created  from  these decompiled files can then be used to train LLMs, allowing the model  to  learn  the  structure  and  features  that  differentiate malicious from benign code. This process, while effective, requires careful handling of the decompiled data to ensure it accurately  reflects  the  behaviour  and  intent  of  the  original malware [3].

While  LLM-based  generative  artificial  intelligence  can significantly augment and enhance the work of cybersecurity professionals, it cannot completely replace their expertise and judgment. Here's how LLM-based GenAI supports cybersecurity teams:

Prompt  and  accurate  threat  detection :  LLMs  can analyse massive amounts of data to identify anomalies and suspicious  patterns,  significantly  reducing  the  time  and resources required to identify potential threats.

Predictive capabilities : By providing the ability to learn from historical data and ongoing threat intelligence, LLMs can predict future attack vectors and vulnerabilities, enabling proactive security measures.

Improved  efficiency  and  productivity :  Generative  AI has  the  potential  to  automate  routine  tasks  such  as  threat analysis and incident response, freeing up valuable time for

human  experts  to  focus  on  strategic  decision  making  and complex investigations [4,5,6].

## II. LARGE LANGUAGE MODELS

LLMs operate through a series of intricate processes that enable them to process and generate human-like text. These processes include tokenization, encoding, positional encoding, transformer layers, and decoding [7, 8].

1. Tokenization: LLMs begin by breaking down input text into smaller units known as tokens, which may represent words, subwords, or characters. Each token is assigned a unique identifier that facilitates processing within the model.
2. Encoding: Tokens are then converted into numerical embedding vectors that encapsulate semantic information, aiding the model's understanding of context and meaning
3. Positional Encoding: To ensure the model recognizes the sequence of tokens, positional encoding incorporates information about the order of tokens, allowing for differentiation based on their relative placement within a sequence.
4. Transformer Layers: LLMs employ transformer architectures, which consist of multiple layers of selfattention mechanisms and feedforward neural networks. The self-attention mechanism enables the model to assess relationships between tokens across an input sequence, capturing complex dependencies and enhancing contextual understanding. The feedforward network then applies transformations to refine token representations further.
5. Decoding: After processing through transformer layers, the model generates output by sampling from a learned probability distribution or employing heuristic search techniques such as beam search to determine the most probable sequence of tokens.

It  should  be  noted  that  while  malware  is  generally considered to be "malicious software" created by the software itself, LLMs have the capacity to detect patterns in software code. For instance, they can be utilised to identify a piece of code as potential malware [9].

Fig. 1. Classification  of  various  methods  through  which  LLMs  can  be deployed.

<!-- image -->

It is important to note that a large number of methods and systems  exist  to  combat  malware,  with  anti-virus  software being one of the most ubiquitous. However, in the current era of  Generative  AI  (GenAI),  LLMs  have  the  potential  to provide  a  competitive  advantage  by  thwarting  malware attacks before or even during their execution. LLMs can be employed in the following manner [9]:

Honeypots: Malware Honeypots: LLMs have the capacity to generate content that is both realistic  and deceptive,  including  phishing  emails,  fake  websites  and documents. These can then be used as bait to lure attackers into honeypots. The interaction with these honeypots facilitates the collection of data that can be utilised to enhance LLM training, thereby enhancing threat detection capabilities.  It  is  possible  to  calculate  the  engagement  of attackers with malware honeypots by simply counting all the interactions of each attacker with each of the honeypots [9].

Text Analysis: LLMs have the capacity to be trained to identify  malicious  code  embedded  within  text  files  that appear  to  be  benign.  For  instance,  malware  may  employ obfuscated  code  within  a  text-based  file  to  circumvent conventional security measures [9].

Code Analysis: LLMs have the capacity to analyse the structure and semantics of code in order to detect potential malicious  intent,  even  in  cases  where  the  code  has  been obfuscated  or  written  in  a  non-standard  way.  The  LLM  is capable of calculating a risk score by means of analysing the code in its entirety or a partial snippet. The LLM is capable of determining the presence of harmful code within the code under scrutiny by utilising the risk score calculated [9].

Trend  Detection: LLMs  have  been  shown  to  process large datasets of past malware, thereby enabling the identification of trends and the prediction of future attacks. This  process  involves  the  aggregation  of  malware  samples exhibiting  similarities  and  the  subsequent  analysis  of  their temporal progression. One such methodology for the identification of trends within a large corpus of malware data is metric-based clustering [9].

Non-standard Disguised Malwar: In the contemporary landscape of cyber-security, where adversaries are becoming increasingly  sophisticated  in  their  methods  of  concealing malware,  the  potential  of  LLMs  to  enhance  the  detection capabilities  through  training  on  adversarial  examples  is  a promising  avenue  for  research.  This  process  involves  the generation of adversarial samples and the refinement of the model  to  enhance  its  robustness.  One  approach  to  this problem involves the use of generative adversarial networks or variational autoencoders to slightly perturb existing malware data. The resulting adversarial samples are then used to train an LLM to minimise the adversarial risk [9].

With the increasing complexity of modern malware, traditional rule-based detection systems are proving inadequate.  Recent  advances  in  LLMs  have  demonstrated significant  potential  in  analysing  source  code,  identifying malicious patterns, and even detecting zero-day vulnerabilities [10, 11]. This section later examines the use of BERT, CodeBERT, GPT-2 and T5 in malware detection.

## A. BERT

BERT,  which  was  originally developed for natural language  processing  tasks,  has  been  fine-tuned  for  source code  analysis  and  malware  detection.  Researchers  have trained BERT on various datasets, achieving an accuracy rate between 91% and 95% when identifying malware patterns. The  model  effectively  detects malicious  function calls, unusual API usage, and disguised malware logic by leveraging its  deep  contextual  understanding.  However, BERT  faces  challenges  in  the  detection  of  obfuscated  or polymorphic malware, where malicious intent is obfuscated through  complex  transformations.  Despite  this  limitation, BERT  remains  a  strong  candidate  for  structured  malware detection, particularly when working with large decompiled datasets.

## B. CodeBERT

CodeBERT  represents  an  extension  of  BERT,  with  its development focusing specifically on programming languages  as  opposed  to  natural  language.  In  contrast  to BERT, which is trained on general text, CodeBERT has been optimised for static malware  analysis,  examining  code structure and function flow to identify threats. When applied to  malware  detection,  CodeBERT  has  demonstrated  an ability to identify malicious code based on API calls, control flow, and function-level patterns. It has been determined that CodeBERT outperforms standard BERT in the analysis of structured code, thus rendering it an excellent choice for static malware  detection.  However,  it  is  important  to  note  that CodeBERT is not without its limitations when dealing with highly obfuscated or encrypted malware binaries, as it relies primarily on recognizable code patterns. Despite this challenge,  CodeBERT  remains  one  of  the  most  effective models for malware detection in decompiled source code.

## C. GPT

GPT  is  a  generative  model  that  focuses  on  pattern recognition  and  anomaly  detection.  Rather  than  relying  on predefined  rules  for  malware  classification,  GPT  has  been trained  on  malware  execution  traces  and  decompiled  code, enabling it to predict suspicious behaviours. When employed in the context of cybersecurity applications,  GPT  has demonstrated  an  89%  accuracy  rate  in  detecting  malware, signifying its potential as a valuable tool for identifying novel threats and previously unseen attack patterns. Its strength lies in  its  ability  to  detect  anomalies  in  behavioural  data,  as opposed  to  relying  on  static  analysis  alone.  However,  its computational  cost  is  considerably  higher  compared  to models such as CodeBERT or BERT, which hinders its largescale  deployment.  While  GPT  can  recognise  malware-like behaviour, it occasionally encounters challenges with highly obfuscated threats, necessitating additional fine-tuning.

## D. T5

T5 adopts a distinct approach in comparison to the other models under discussion, as it reformulates the classification of malware as a text-to-text generation problem. Rather than merely categorising code as malicious or benign, T5 is trained to generate textual descriptions of potential malware behaviour, a method that renders it highly flexible due to its capacity to process multiple input types. Notwithstanding its adaptability, T5 has demonstrated marginally lower accuracy in  comparison  to  CodeBERT  and  BERT.  However,  its capacity to process diverse input formats confers a distinct advantage in cybersecurity applications that demand a more extensive contextual understanding.

## E. Model Selection

Whilst BERT is a powerful model for malware detection, the decision to utilise T5, GPT and CodeBERT was driven by their  combined  ability  to  effectively  handle  threat  patterns. Due  to  time  limitations,  we  were  only  able  to  train  three models, so selecting the most adaptable and effective models was critical.

The  selection  of  models  was  prioritised  based  on  their ability to offer comprehensive and flexible malware detection, given  the  constrained  timeframe  available  for  training.  The integration  of  GPT's  anomaly  detection,  T5's  generative approach, and CodeBERT's structured code analysis collectively  form  a  robust  framework  for  identifying  both known and emerging threats. While BERT has been shown to be highly effective in other studies, we opted not to train it, as CodeBERT has been demonstrated to offer similar strengths with additional optimisations for code-based threats.

## III. DATASET AND LLM SETUP

In this section we will provide information about Dataset we used and the LLM setup.

## A. Dataset

The dataset under study consists of real malware files and PE files that are commonly used and do not contain malware. The PE files were subsequently converted to C code using the Ghidra  tool.  Consequently,  the  resulting  dataset  comprises two distinct categories of code: namely, Malware and Benign. The dataset used for training is shown in the Table I.

TABLE I. TRAIN DATASET

| Dataset name   | Number of files   | Number of files   |
|----------------|-------------------|-------------------|
| Benign         | 15,000            | 15,000            |
| Malware        | 15,000            | 15,000            |
| Total count :  | Total count :     | 30,000            |

Despite the uniform selection of the training dataset, the test dataset was selected unevenly due to the fact that malware is  encountered  less  frequently  in  normal  use  than  benign applications. Consequently, the test dataset was selected at a ratio of 5:1 (benign: malware). The composition of the test dataset is shown in the Table II.

TABLE II. TEST DATASET

| Dataset name   | Number of files   | Number of files   |
|----------------|-------------------|-------------------|
| Benign         | 871               | 871               |
| Malware        | 129               | 129               |
| Total count :  | Total count :     | 1,000             |

## B. LLM Setup

In the training of the models, those with an equivalent or analogous number of parameters were selected. The

distribution  of  parameters  is  illustrated  in  the  table  located beneath the text.

CodeBERT is comparable to GPT-2 Small, but T5 does not have a comparable amount of parameters. It has something around 60 million in the Small version and 220 million in the Base version. Overall information is shown in Table III

TABLE III. LLM PARAMETERS

| Feature / Setup         | CodeBERT                            | GPT-2                               | T5                                  |
|-------------------------|-------------------------------------|-------------------------------------|-------------------------------------|
| Model Used              | microsoft/codebert- base            | gpt2                                | t5-base                             |
| Tokenize r              | RobertaTokenizer                    | GPT2Tokenizer (+ eos_token padding) | T5Tokenizer                         |
| Paramete rs             | 125 million                         | 124 million                         | 220 million                         |
| Model Architect ure     | RobertaForSequenc eClassification   | GPT2ForSequence Classification      | T5ForSequenceC lassification        |
| Input                   | .c Code                             | .c Code                             | .c Code                             |
| Tokeniza tion Strategy  | Max length 512, padding, truncation | Max length 512, padding, truncation | Max length 512, padding, truncation |
| Class Weights           | not calculated - used default       | not calculated - used default       | not calculated - used default       |
| Train/Val idation Split | 80/20                               | 80/20                               | 80/20                               |
| Evaluatio n Metrics     | None                                | None                                | None                                |
| Epochs                  | 5                                   | 5                                   | 5                                   |
| Batch Size              | 8                                   | 8                                   | 8                                   |
| FP16 Training           | Enabled                             | Enabled                             | Enabled                             |

## IV. RESULTS

We tested the models on a test dataset. The training results are shown below. We have provided the testing results in a confusion matrix table.

## A. GPT 2

We  evaluated the performance of our LLM-based malware detection model using GPT 2 on a test set of 1,000 samples. The dataset included 129 malware samples and 871 benign samples. The results of the evaluation are shown in Table IV and confusion matrix in Figure 2.

Fig. 2. Confusion Matrix GPT 2.

<!-- image -->

TABLE IV. GPT 2 RESULTS

| Metric                                        | Value   |
|-----------------------------------------------|---------|
| Total Samples                                 | 1000    |
| Total Malware Samples                         | 129     |
| Total Benign Samples                          | 871     |
| True Positives (Malware correctly classified) | 109     |
| False Negatives (Malware missed)              | 20      |
| True Negatives (Benign correctly classified)  | 724     |
| False Positives (Benign misclassified)        | 147     |
| Total Misclassifications                      | 276     |
| Malware Detection Accuracy (Recall)           | 84.5%   |
| Overall Accuracy                              | 72.4%   |
| False Negative Rate                           | 15.5%   |
| Precision                                     | 42,58%  |
| F1 Score                                      | 56,62%  |

The precision, recall, F1 Score and accuracy for GPT 2 are all merged in Figure 3.

Fig. 3. Precision, Recall, F1 Score and Accuracy for GPT 2.

<!-- image -->

The model  demonstrates a strong ability to detect malware,  with  a  recall  of  84.5%,  meaning  it  correctly identifies most malicious samples. However, the precision is relatively low at 42.6%, indicating that a significant number of  benign  files  are  incorrectly  classified  as  malware.  The overall accuracy  of 72.4%  is  acceptable  but could  be improved by reducing false positives. The F1 score of 56.6% reflects  a  moderate  balance  between  precision  and  recall. Overall, the  model performs well in identifying threats but requires further refinement to enhance reliability and reduce misclassification of benign software.

## B. T5

To evaluate our T5-based LLM for malware detection, we conducted testing on a dataset of 1,000 samples, consisting of 129  malicious  and  871  benign  instances.  The  performance outcomes are detailed in Table V, and the confusion matrix is depicted in Figure 4.

Fig. 4. Confusion Matrix T5.

<!-- image -->

TABLE V.

T5 RESULTS

| Metric                                        | Value   |
|-----------------------------------------------|---------|
| Total Samples                                 | 1000    |
| Total Malware Samples                         | 129     |
| Total Benign Samples                          | 871     |
| True Positives (Malware correctly classified) | 110     |
| False Negatives (Malware missed)              | 19      |
| True Negatives (Benign correctly classified)  | 612     |
| False Positives (Benign misclassified)        | 259     |
| Total Misclassifications                      | 278     |
| Malware Detection Accuracy (Recall)           | 85.3%   |
| Overall Accuracy                              | 72.2%   |
| False Negative Rate                           | 14.7%   |
| Precision                                     | 29.8%   |
| F1 Score                                      | 44.2%   |

The precision, recall, F1 Score and accuracy for T5 are all merged in Figure 5.

Fig. 5. Precision, Recall, F1 Score and Accuracy for T5.

<!-- image -->

The model achieves a high recall of 85.3%, indicating that it is effective in identifying most malware samples. However, its  precision  is  low  at  29.8%,  showing  that  it  frequently misclassifies  benign  files  as  malicious.  With  an  overall accuracy of 72.2%, the model demonstrates consistent performance  in  total  classifications  but  suffers  from  a  high false  positive  rate.  The  F1  score  of  44.2%  highlights  the imbalance  between  detection  sensitivity  and  classification precision.  Overall,  while  the  model  is  strong  at  catching malware,  its  reliability  is  limited  by  the  high  number  of incorrect  alerts,  suggesting  the  need  for  precision-focused improvements.

## C. Code BERT

The performance of our CodeBERT-based large language model  for  malware  detection  was  evaluated  using  a  test dataset  comprising  1,000  samples,  including  129  malware instances and 871 benign instances. The detailed evaluation results  are  presented  in  Table  VI,  with  the  corresponding confusion matrix provided in Figure 6.

Fig. 6. Confusion Matrix CodeBERT.

<!-- image -->

## TABLE VI. CODEBERT RESULTS

| Metric                                        | Value   |
|-----------------------------------------------|---------|
| Total Samples                                 | 1000    |
| Total Malware Samples                         | 129     |
| Total Benign Samples                          | 871     |
| True Positives (Malware correctly classified) | 129     |
| False Negatives (Malware missed)              | 0       |
| True Negatives (Benign correctly classified)  | 0       |
| False Positives (Benign misclassified)        | 871     |
| Total Misclassifications                      | 871     |
| Malware Detection Accuracy (Recall)           | 100.0%  |
| Overall Accuracy                              | 12.9%   |
| False Negative Rate                           | 0.0%    |
| Precision                                     | 12.9%   |
| F1 Score                                      | 22.9%   |

The precision , recall F1 Score and accuracy for CodeBERT are all merged in Figure 7.

Fig. 7. Precision, Recall, F1 Score and Accuracy for CodeBERT.

<!-- image -->

## V. SUMMARY

With this work, we aimed to investigate the potential of the LLMs model in malware detection. This section summarizes and compares the performance of three LLM-based malware detection models: GPT, T5, and CodeBERT. It includes key metrics, advantages, and disadvantages for each model. The summary results are shown in Table VII.

TABLE VII. SUMMARY RESULTS

| Metric                   | GPT 2 Model   | T5 Model   | CodeBERT Model   |
|--------------------------|---------------|------------|------------------|
| Total Samples            | 1000          | 1000       | 1000             |
| Malware Samples          | 129           | 129        | 129              |
| Malware Detected (TP)    | 109           | 110        | 129              |
| Malware Missed (FN)      | 20            | 19         | 0                |
| Total Misclassifications | 276           | 278        | 871              |
| Recall                   | 84.5%         | 85.3%      | 100.0%           |
| Precision                | 42.6%         | 41.7%      | 12.9%            |
| F1 Score                 | 56.4%         | 56.2%      | 22.9%            |
| Overall Accuracy         | 72.4%         | 72.2%      | 12.9%            |
| False Negative Rate      | 15.5%         | 14.7%      | 0.0%             |

LLMs such as GPT-2 and T5 have been demonstrated to achieve effective performance in the malware detection task. Conversely, CodeBERT  classified all input samples as malware, indicating a substantial bias in its predictions.

In  the  present  study,  a  rigorous  experimental  setup  was implemented,  ensuring  the  integrity  of  the  data.  This  setup involved the careful reading and tokenization of the data, the utilisation of a custom CodeDataset class to process the input, and the training of the model using the RobertaForSequenceClassification architecture. The implementation also involved the computation and application of balanced class weights to address potential label imbalance, in  addition  to  the  splitting  of  the  dataset  for  training  and evaluation purposes, utilising a standard 80/20 approach.

Despite  this  rigorous  methodological  framework,  the CodeBERT model failed to distinguish between benign and malicious code samples. Consequently, it can be concluded that  the  CodeBERT  model is  not  an  optimal  model  for  the specific dataset and problem formulation under investigation, unless  further  adaptation  or  architectural  modifications  are made.

This study builds on our previous research [12, 13, 14, 15, 16], which involved a comparative analysis of low-level and high-level approaches to malware detection. In this study, we extended  our  investigation  by  evaluating  several  LLMs  on real-world malware samples.

The findings underscore the efficacy of models such as T5 and GPT in the effective identification of malicious code. In the subsequent phase of research, the objective is to enhance the robustness of these models through the training of more sophisticated T5 or GPT architectures on substantially larger and more diverse datasets. Table VIII shows final comparsion of tested LLM models.

TABLE VIII. FINAL COMPARISON

| Model    | Advantages                                                                                                                    | Disadvantages                                                                                                                |
|----------|-------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| GPT 2    | - Good balance of precision and recall - Decent overall accuracy (72.4%) - Low false negative rate for malware                | - 15.5% of malware samples missed - Moderate false positive rate (147 benign misclassified)                                  |
| T5       | - Slightly better malware detection than GPT (85.3% recall) - Similar performance to GPT - Slightly lower false negative rate | - High false positives (168 benign misclassified) - Modest F1 score and precision (~41.7%)                                   |
| CodeBERT |                                                                                                                               | - Extremely high false positive rate (871 benign misclassified) - Very low overall accuracy (12.9%) - Poor precision (12.9%) |

## ACKNOWLEDGMENT

The  paper  has  been  supported  by  the  outputs  of  the research project "VV10 - Use of commercial unmanned aerial vehicles with a range of up to 5 km to conduct reconnaissance, targeting, and for carrying and dropping explosive elements" funded  by  the  Ministry  of  Defence  of  the  Slovak  Republic through  the  inter-ministerial  sub-program  06E0I-Research and development in support of state defence.

## REFERENCES

- [1] Y.  Yigit,  W.J.  Buchanan,  M.G.  Tehrani,  L.  Maglaras,  "Review  of Generative  AI  Methods  in  Cybersecurity", arXiv.org ,  2024,  DOI: https://doi.org/10.48550/arXiv.2403.08701.
- [2] C.  Patsakis,  F.  Casino,N.  Lykousas. 'Assessing  LLMs  in  Malicious Code  Deobfuscation  of  Real-World  Malware  Campaigns.'  Expert systems with applications , 2024 DOI: doi.org/10.48550/arXiv.2404.19715.
- [3] N. Maloyan, E. Verma, B. Nutfullin, B.. Ashinov,  'Trojan Detection in  Large  Language  Models:  Insights  from  The  Trojan  Detection Challenge.' arXiv.org , 2024,DOI: https://doi.org/10.48550/arXiv.2404.13660.
- [4] S. Metta, I. Chang, J. Parker, M.P. Roman, A.F. Ehuan, 'Generative AI in Cybersecurity.' arXiv.org , 2024, DOI: https://doi.org/10.48550/arXiv.2405.01674.
- [5] P.M.S.  Sanchez,  A.H.  Celdrán,  G.  Bovet,  G.M.  Pérez,  'Transfer Learning in Pre-Trained Large Language Models for  Malware Detection Based on System Calls.' MILCOM  IEEE Military Communications Conference , pp 853-858, 2024 DOI: https://doi.org/10.48550/arXiv.2405.09318.
- [6] M. Naseer, F. Ullah, S. Iljaz, H. Naeem, A. Alsirhani, G.N. Alwakid, A.  Alomari,  'Obfuscated  Malware  Detection  and  Classification  in Network  Traffic  Leveraging  Hybrid  Large  Language  Models  and Synthetic Data.' Sensors (Basel, Switzerland) , 2025, DOI: https://doi.org/10.3390/s25010202.

- [7] J. Nelson, M.  Pavlidis, A. Fish, S. Kapetanakis, N. Polatidis, 'ChatGPT-Driven  Machine  Learning  Code  Generation  for  Android Malware Detection.' Computer journal , 2024, DOI: https://doi.org/10.1093/comjnl/bxae114.
- [8] A.A.  Hossain,  M.  Kumar,  Z.  Junjie,  A.  Fathi,  'Malicious  Code Detection Using LLM.' NAECON 2024 - IEEE National Aerospace and Electronics Conference , pp. 414-416, 2024, DOI: 10.1109/NAECON61878.2024.10670668.
- [9] J.  Al-Karaki,  M.  Al-Zafar  Khan,  M.  Omar.  'Exploring  LLMs  for Malware Detection: Review, Framework Design, and Countermeasure Approaches.' arXiv.org , 2024, DOI: https://doi.org/10.48550/arXiv.2409.07587.
- [10] M.  Amin,  B.  Shah,  A.  Sharif,  T.A.  Tanveer,  K.  Kim,  S.  Anwar 'Android Malware Detection through Generative Adversarial Networks.' Transactions on emerging telecommunications technologies 33.2, 2022, DOI: 10.1002/ett.3675.
- [11] Y. Awad, M. Nassar, H. Safa. 'Modeling Malware as a Language.', IEEE  International  Conference  on  Communications  (ICC) ,pp.  1-6, 2018, DOI: 10.1109/ICC.2018.8422083.
- [12] M. Adamec, M. Turcanik, "Malware Signatures Detection with Neural Networks.", pp 1-8, 2022, DOI:10.23919/NTSP54843.2022.9920380.
- [13] M.Adamec, M. Turcanik, "Comparing Signature Detection of Convolutional  Neural  Network  in  Low-Level  and  Large  Language Model in High-Level Programming Language.", pp 1-8, 2023, DOI: 10.1109/KIT59097.2023.10297113.
- [14] M.  Adamec,  M.  Turcanik,  "Development  of  Malware  Using  Large Language Models.", pp 1-5, 2024, DOI: 10.23919/NTSP61680.2024.10726304.
- [15] J. Baráth and M. Líška, "Use of Data Mining Techniques for Network Data Analysis," 2021 Communication and Information Technologies (KIT) , Vysoke Tatry, Slovakia, 2021, pp. 1-6, doi: 10.1109/KIT52904.2021.9583755.
- [16] M. Obert and M. Harakal, "Fuzzy Inference System Application On High  Level  Security  Model," 2019 Communication and Information Technologies  (KIT) ,  Vysoke  Tatry,  Slovakia,  2019,  pp.  1-9,  doi: 10.23919/KIT.2019.8883466.