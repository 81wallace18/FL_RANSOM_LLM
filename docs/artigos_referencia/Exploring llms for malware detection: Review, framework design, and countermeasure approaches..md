## Exploring LLMs for Malware Detection: Review, Framework Design, and Countermeasure Approaches

Jamal Al-Karaki 1,2 , Muhammad Al-Zafar Khan 1* , Marwan Omar (Member, IEEE) 3

1* College of Interdisciplinary Studies (CIS), Zayed University, Abu Dhabi, UAE.

2 College of Engineering, The Hashemite University, Zarqa, Jordan.

3 Department of Information Technology and Management, Illinois Institute of Technology, Chicago, IL, USA.

*Corresponding author(s). E-mail(s):

Muhammad.Al-ZafarKhan@zu.ac.ae;

Contributing authors: Jamal.Al-Karaki@zu.ac.ae; momar3@iit.edu;

## Abstract

The rising use of Large Language Models (LLMs) to create and disseminate malware poses a significant cybersecurity challenge due to their ability to generate and distribute attacks with ease. A single prompt can initiate a wide array of malicious activities. This paper addresses this critical issue through a multifaceted approach. First, we provide a comprehensive overview of LLMs and their role in malware detection from diverse sources. We examine five specific applications of LLMs: Malware honeypots, identification of text-based threats, code analysis for detecting malicious intent, trend analysis of malware, and detection of non-standard disguised malware. Our review includes a detailed analysis of the existing literature and establishes guiding principles for the secure use of LLMs. We also introduce a classification scheme to categorize the relevant literature. Second, we propose performance metrics to assess the effectiveness of LLMs in these contexts. Third, we present a risk mitigation framework designed to prevent malware by leveraging LLMs. Finally, we evaluate the performance of our proposed risk mitigation strategies against various factors and demonstrate their effectiveness in countering LLM-enabled malware. The paper concludes by suggesting future advancements and areas requiring deeper exploration in this fascinating field of artificial intelligence.

Keywords: Cybersecurity, Large Language Models (LLMs), Malware Detection, Risk Mitigation Strategies, Performance Metrics, Threat Analysis

## 1 Introduction

The swift evolution of malware threats poses significant challenges to cybersecurity. Conventional malware detection techniques often struggle to keep up with the sophisticated and polymorphic malware that continually adapts to evade detection. Recent advancements in Natural Language Processing (NLP), particularly through the use of Large Language Models (LLMs), present a promising new approach to addressing these challenges. LLMs, with their advanced capabilities in understanding and generating human-like text, have demonstrated exceptional performance across various domains, including text classification and anomaly detection. This paper explores the potential of leveraging LLMs for malware detection via articulating a systematic review of the state-of-the-art in the field, focusing on their ability to analyze and interpret complex patterns in executable code and network traffic. By harnessing the power of LLMs, we aim to enhance and shed light on the in-vogue practices that exist and characterize the various literature pieces that exist in a consolidated and sequential format, offering a novel perspective on combating the evolving threat landscape.

In particular, a comprehensive introduction to LLMs and malware and how, in general, LLMs can be used to detect malware attacks since its advent in November 2022, ChatGPT, released by OpenAI [1-3], has been used by consumers from all backgrounds and for various domains. Subsequently, many other big tech companies have pooled their resources and strategically directed their research teams to create and put into production their own versions of LLMs. In general, LLMs work as follows:

1. Tokenization: Input text (prompts) are broken down into small units called tokens and assigned a unique integer value that is used for identification of position in the LLM's underlying model. Essentially, there are three primary tokenization methods: For words, for subwords, and for characters.

Given the input prompt 'This is my prompt', one potential tokenization is

<!-- formula-not-decoded -->

2. Encoding: Tokens are converted to numerical embedding vectors that capture semantic information about the tokens. This facilitates the model's understanding of the contextual nature of the tokenized words.

A potential encoding of the above tokens is

<!-- formula-not-decoded -->

3. Positional Encoding: In order to incorporate positional information of the numerical embeddings, positional encoding is performed in order to provide the underlying LLM model with information about the order of the tokens in the input sequence. This facilitates the differentiation between tokens based on their relative

dispositions. Mathematically, positional encoding is given by

<!-- formula-not-decoded -->

where ⟨ i, j ⟩ is the position of the token in the sequence, k is the dimension index of the embedding vector, and d is the dimensionality of the token embeddings.

4. Transformer Layer: The transformer layer is composed of feedforward neural network layers that use the self-attention mechanism to compare the input sequences to each other in a hierarchical manner. This allows for context to be captured effectively, and the encapsulation of long-range dependencies. Mathematically, the self-attention mechanism is given by

<!-- formula-not-decoded -->

where Q is the query matrix, K is the key matrix, d k is the dimensionality of the key vector, V is the value matrix, and σ ( ξ i ) = exp ( ξ i ) / ∑ j exp( ξ j ) is the softmax activation function, with ξ being a placeholder.

The output from the attention mechanism is then fed into the feedforward neural network, which applies a nonlinear transformation to the encoded tokens in order to develop complex relationships that are not possible with the assumption of simple linear relationships; commonly, this is chosen to be the ReLU function. Mathematically, the application of the feedforward neural network yields

<!-- formula-not-decoded -->

where x is the tokenized input embedding, W ℓ are the weights in the ℓ th layer, b ℓ are the bias terms in the ℓ th layer, L is the number of layers, and ReLU( η ) = max(0 , η ) is the ReLU activation function, with η being a placeholder.

The steps 1 -5 are summarized in the architecture diagram Fig. 1.

An important feature of LLMs is that they contain multiple transformer layers stacked. The output from one layer becomes the input to the next layer. The purpose of stacking is for the refinement of the tokens, and to increasingly capture complex patterns from the data.

5. Decoding: Upon passing through the stacked transformer layers and undergoing refinement, the decoder samples from the output distribution or uses a heuristic search algorithm like beam search to select the most probable sequence of tokens.

Since the launch of ChatGPT, every aspect of human life has been affected. From education to internet search, to science, law, medicine, and even research, there has been a rise in people using it as a tool to achieve their objectives, be it for casual use

<!-- image -->

Output Distribution

Fig. 1 A simplified overview of a Large Language Model (LLM) pipeline. While it is a gross oversimplification of the intricate architecture and operations of a real LLM, it effectively illustrates the core steps involved in generating text responses, namely: Tokenization, encoding, positional encoding, various stacked layers, the attention mechanism, the output distribution, and the most probable sequence generation.

or in their work. The latest version, as of March 2024, powered by the GPT-4 engine, has been demonstrated to perform substantially well in major benchmarking examinations, consistently scoring in the upper percentiles, like the New York Bar Exam (Law) [4], the SAT exam for aspirant US-college entrants, various Advanced Placement (AP) subject examinations [5], orthopedic in-training examinations [6], United States Medical Licensing Exam (USMLE) [7, 8], the University of Pennsylvania's Wharton School of Business MBA exam [9, 10], a radiology-style board exam [11], and is currently being tested out to determine its performance on other standardized examinations that are notoriously difficult; a more comprehensive discussion of multiple choice-type examinations passed by ChatGPT is provided in [12].

Additionally, many businesses have been built on LLMs; for example, there are sites that offer to build entire presentation decks on any topic or generative models that can generate images and video based on text prompts and an overabundance of other applications; see, for example, [13] for the business impact. This has led to a division in society and amongst experts as to whether it's something good to have in the first place, and more especially to have them accessible to members of society engaged in foundational and critical tasks like children at school, as the arguments

posited is that it leads to children not being innovative and stretching their minds and imaginations to think critically and originally.

A characterization of how 'powerful' an LLM is, is by the number of parameters that it has. Table 1 tabulates some of the most common examples of LLMs.

Table 1 Table 1: Benchmarking of some popular LLMs.

| LLM           | Parent Company                  | Number of Parameters                    |
|---------------|---------------------------------|-----------------------------------------|
| ChatGPT       | OpenAI                          | GPT-3: 175 × 10 9 GPT-4: 1 . 76 × 10 12 |
| Gemini / Bard | DeepMind                        | 30 - 65 × 10 12                         |
| Falcon        | Technology Innovation Institute | 180 × 10 9                              |
| BloombergGPT  | Bloomberg                       | 50 × 10 9                               |
| LLaMa         | Meta                            | 65 × 10 9                               |
| Grok          | X.ai                            | 314 × 10 9                              |
| BERT          | Google                          | 345 × 10 6                              |
| PaLM          | Google                          | 540 × 10 9                              |
| Orca          | Microsoft                       | 13 × 10 9                               |
| Claude        | Anthropic                       | 137 × 10 9                              |

Originating from ideas in NLP, nowadays, LLMs use Deep Learning (DL) models, like transformers, to find statistical relationships between words and phrases in its training data/corpus. While LLMs can seemingly answer any question, they oftentimes suffer from the hallucination problem [14-16] in which they make up information that seems correct but is completely fallacious. While unlocking nearly infinite possibilities in their ability to answer questions on the go, LLMs are not Artificial General Intelligence (AGI) and, thus, cannot autonomously think and make decisions for themselves and, therefore, require prompting.

malware , the portmanteau of the words 'malicious' and 'software' is a software program that is specifically designed to cause harm to a computer system/network. Hackers have used malware to disrupt operations in a computer system, using the software to overwork the hardware and therefore causing damage to the hardware, stealing data, and even spying, and is therefore viewed as a major threat to society, governments, and can potentially cause global disorder due to the financial implications, and the leaking of sensitive information caused by data breaches. Computing security experts have categorized malware into the following groupings:

1. Worms: Hostile self-replicating programs that spread throughout the computing system without attaching to other programs.
2. Spyware: Using to steal critical information. These include passwords, personal data, login credentials for online banking, and so on.
3. Ransomware: Programs that restrict, or completely block, access to a computing system. Oftentimes, they block access to certain files, and the hacker behind its deployment demands monetary rewards (cash or untraceable cryptocurrencies) in order to restore access.
4. Viruses: Programs that replicate themselves and maliciously couple to other programs with the goal of spreading and infecting the computer as they are being used.

5. Trojan Horses: Programs that disguise themselves as legitimate programs from reputable companies, with the intention of being accessed in order to cause malicious damage to its host.

In Table 2, we juxtapose how various LLMs fair in comparison to one another for the task of malware detection.

Table 2 A comparison of various LLM models for malware detection.

| LLM             | Performance                                                                                                                                                                                                                                     |
|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| BERT            | • Malware samples are represented by a sequence of tokens. • Contextual information is encapsulated effectively due to its bidirectional nature. • BERT shows superior performance when detecting subtle malware patterns.                      |
| RoBERTa         | • An optimized version of BERT that addresses some of BERT's limitations when it comes to NLP tasks. • Potentially offers advanced performance when compared with BERT due to the robust training methodology incorporated in its architecture. |
| AlBERTa         | • A compact/lite variation of BERT that achieves analogous performance but has fewer parameters. • Highly applicable for detecting malware in resource-constrained environ- ments. This is because it uses less computational resources.        |
| GPT-2 and GPT-3 | • These LLM models are trained on a large corpus of text data. • Due to the rigorous training process, these models can generate features for subsequent analysis or be fine-tuned to detect malware effectively.                               |
| XLNet ∗         | • Not very often used for malware detection tasks. • Due to its advanced generalized autoregressive pretraining, it outperforms BERT and its variants on many other NLP tasks. Thus, it can potentially be used for enhanced performance.       |

The foremost objective of this paper is to discuss, in sufficient detail, how LLMs can be used to detect camouflaged patterns that may exist within code that may be nearly impossible for an expert system - like a human - to detect. We are conscious of the fact that the construction of a survey paper in this field is not novel; however, the purpose of this paper is to give accounts of the latest developments in the field - Particularly how LLMs are being used for malware detection, address various shortcomings of the current frameworks that exist in order to provide an attentions of these risks and target a broader scope of security researchers interested in the field. The novel contributions of this paper are:

1. A comprehensive exploration of the various literature pieces in the field.
2. Apropoundment of a set of guiding principles to steer LLMs in the correct direction for detecting malware.
3. The proposal of a risk mitigation framework on leveraging LLMs for detecting malware from software.
4. A discussion on the faux pas related to using LLMs for detecting malware from software.

The rest of the paper is organized as follows:

- In Sec. 2, we provide an organized and exhaustive coverage of the research in the field.
- In Sec. 4, we propose a set of guidelines and risk mitigation strategies for preventing malware attacks using LLMs.
- In Sec. 3, we use our understanding from the literature to formulate metrics for using LLMs as a tool to prevent different attack types.
- In Sec. 5, we demonstrate the use of the metrics from Sec. 3 with fictitious data that has the potential to be collected in the real world.
- In Sec. 6, we conclude on the findings of this paper and provide a discussion on why LLMs cannot possibly prevent all malware attacks.

## 2 Literature Review

In this section, we peruse the various literature sources relevant to the core objective of using LLMs for malware detection.

In order to classify the literature into buckets with common trends, we use the following scheme to denote the various characterizations described in Tab. 3.

Table 3 Description of the various symbols used for characterizing the different literature papers.

| Parameter/Symbol   | Definition                                                      | Description                                                                                                      |
|--------------------|-----------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| α                  | Using LLMs for harmful con- tent generation.                    | Refers to those papers where LLMs are used for creating content that is detri- mental.                           |
| β                  | Exploitation of LLMs for mali- cious weaponization.             | Those papers where LLMs have been, or have the potential to, be utilized for militarization.                     |
| γ                  | The dissemination of malware through, or by the usage of, LLMs. | Those papers that use, or have the potential to use, LLMs for the distri- bution of malware.                     |
| δ                  | Survey papers of the field.                                     | Those research papers that provide a review of the existing research in the field of LLMs for malware detection. |
| ϵ                  | Benchmarking LLMs on mal- ware.                                 | Those papers that test out various LLMs on different malware in order to provide a gauge.                        |
| ζ                  | Proposal of a new and effective model.                          | Those papers that advance new LLMs for malware detection.                                                        |
| *                  | Out of the ordinary paper, but still relevant.                  | Those papers that are atypical of the research in the field of LLMs for mal- ware detection.                     |

Below, we provide a review of the most prominent pieces of work on the subject matter; this comprises survey papers and contemporary works in the field.

In[17] ( δ ), the authors explore the positive impact of LLMs on security and privacy, prospective threats, and the susceptibilities of LLMs. This work characterizes the various papers into three types: Those with beneficial applications, those with offensive applications, and those having vulnerabilities and defenses.

In [18] ( α , β , γ ), the authors present a proof-of-concept that illustrates the usage of ChatGPT as an agent for delivering malware while evading detection. With a particular emphasis on command-and-control (C2) servers, the authors underscore how to leverage LLMs for executing commands on the victim's computer. Lastly, the paper explores the need for risk mitigation strategies, frameworks, and guidelines that need to be developed and adopted as a preventative measure.

In [19] ( δ ), the authors focus on the intersection of NLP and security, with a specific emphasis on adversarial attacks on nefarious attacks that can mislead LLMs. The paper characterizes the literature papers into various attack types, namely: Textual attacks, multi-modal attacks, and attacks on sophisticated systems like federated and self-organizing (multi-agent) systems. This paper furnishes an examination of various attack types and makes the field more accessible to researchers and laypeople unfamiliar with the topic.

In [20] ( δ ), the authors focus on the defensive and antipathetic use of LLMs in cybersecurity and present a SWOT-like analysis of the state-of-the-art, with a particular focus on understanding the extent of risks and opportunities. The added value of this research is the identification of research gaps like limited defensive applications, scalability, robustness, explainability, data privacy and ethical considerations, and adoption.

In [21] ( α ), the authors focus on simulating human-aided post-breach attacks on LLMs. The richness of this research lies in its scope and focus; whereas other papers in the field focus on the pre-breach (such as malware and phishing) phase of an attack, the sole focus of this research is after a breach occurs.

In [22] ( ϵ ), the authors use two production-grade LLMs, LLaMa-12B, and IDA Pro, to investigate the strategic shift from crypto-ransomware to data-theft ransomware. Specifically, the two LLMs in question are used to emulate and analyze ransomware variants. The findings underscored the necessity for adaptive cybersecurity strategies incorporating advanced detection systems to recognize ransomware activities effectively.

In [23] (*), while strictly not a paper focused on LLMs for malware detection, focuses on the generation of synthetic/inorganic malware-based data using Generative Adversarial Attacks (GANs). The importance of this work cannot be overstressed as data that shows how LLMs can be used to detect malware in code is scant and not widely available because of the exposure of personalized intellectual property (code) of organizations. The research shows that GANs are very effective in generating realistic cyberattacks, and thereafter Deep Learning (DL) models are effective in their classifications. This work lays the foundation for the development of algorithms for using LLMs to detect malware attacks.

In [24] ( α ), the authors use Turing exploration into ML for generating complex and evolving cybersecurity scenarios for LLMs to detect by exploiting their generative ability. This research focuses on the transformation of the impediment of the hallucination problem of LLMs into an advantage for creating new scenarios for the LLM to train on.

In [25] ( ϵ ), the authors use LLaMa-7B to test the proof-of-concept of a novel ransomware detection methodology that involves the analysis of portable executable

Fig. 2 Visual taxonomy of the overlapping categories of classifications within the literature base. Each colored circle represents a distinct category, and the overlapping regions indicate the shared literature pieces between these categories.

<!-- image -->

(PE) files. Specifically, the procedure involves the conversion of the PE files into grayscale bitmap images and then examining them using LLaMa-7B. The highlight of this research was the approach adopted.

In [26] ( β , ϵ ), the authors focus on the identification of the risks associated with indirect prompt injection attacks on LLMs. The research highlights that strategic prompting can influence the behavior of LLMs and remotely capitalize on LLM predisposition. The novelties of this research lie in its overarching gradation of attack vectors: Worming, data theft, ecosystem contamination, etc., and by the viability of real-world applications using GPT-4 (on synthetic data) and prompt injection on Bing Chat and code-completion engines (organic data). Similar findings were established in [27].

In [28] ( α ), the authors explore how LLMs can be used for both positive and negative activities; the dual-use risk . The study demonstrates how instructionfollowing LLMs can produce targeted malicious content (spam, hate speech, etc.) and get around defenses set up on LLM APIs. The findings underscore the need for innovative approaches to preventing and neutralizing the risks posed by instructionfollowing LLMs, and addressing the challenges presented by increasingly sophisticated adversaries and attacks.

In [29] ( α , β ), the author focuses on the usage and potential of LLMs for the creation of metamorphic malware - Malware that automatically adjusts its code for evading detection. The paper introduced a framework, based on LLM architecture, for

creating programs that test themselves for mutation. The paper is also future-focusing in that it focuses on the next generation of malware detection.

In [30] ( ζ ), the authors address a primary concern of malware detection: How to detect it when it continuously evolves to evade identification. By combining hexand op-code features with LLMs, they propose a new model that generates synthetic samples reminiscent of evasive malware. The experimental results indicate that the model showed substantial performance. However, further improvements are required to achieve higher. The novelty of the research, besides the new model, is the particular focus on evasion tactics like obfuscation and encryption.

In [31] ( α , *), the authors focus on the challenges and risks associated with LLMs being used for generating malicious content. The research reasons that semantic censorship needs to be reexamined in the context of being a security threat. Admittedly, this paper has indirect relevance because it helps influence the preventative principles that are posited in Sec. 4.2.

In [32] ( ϵ ), the authors use an ML model powered by ChatGPT to detect phishing emails. By analyzing real-world phishing emails, the research identifies psychological tactics, technical deception, and language anomalies that are habitually used to plant ransomware within organizations.

In [33] ( α , γ ), the authors focus on the generation of malware using ChatGPT, text-davinci-003, and AutoGPT. Using approximately 400 lines of code, seven malware programs, and two attack tools within a short timeframe of 90 minutes, including debugging time, were developed. This demonstrates that within such a short time period, and within the wrong hands, the destructive capability of these models, once again highlighting the dual-use risk discussed in [28]. Lastly, the authors provide discernment into the current barriers and areas for improvement in AI safety mechanisms in LLMs.

In [34] ( β , γ ), the foremost objective of the authors is to determine the relationship between ChatGPT and cyberattacks. In particular, they probe how malicious actors exploit ChatGPT to launch various cyberattacks. It was found that malicious actors posed as regular users to manipulate the model's vulnerability to hostility. The novelties of this research include the identification of tactics employed by malicious actors and an investigation into user perceptions, with the goal of identifying the weaknesses and biases and proposing mitigation strategies.

In [35] ( β , γ , ζ ), the authors investigate how ChatGPT can be used to automate various stages of a phishing attack, and automatically deploy malware to execute these attacks. The novelties of this research include: Generating an automated phishing kit and raising awareness about LLM misuse.

In [36] ( α ), the authors focus on using LLMs to hack websites. The research identifies the empowering strengths that LLMs have that facilitate the hacking process: The ability to read documents, interact with tools, and recursively call themselves. The paper argues that frontier models, like GPT-4, through their advanced features and adeptness, can drive cybersecurity capabilities.

In [37] ( α , β , γ ), the authors examine the evolution of GenAI in the context of cybersecurity. The research demonstrates attacks such as jailbreaks, reverse psychology, and prompt injection on ChatGPT. In particular, how LLMs and general GenAI

tools develop various cyber attacks centering around social engineering, phishing, automated hacking, attack payload generation, malware creation, and polymorphic malware. This research exposes ChatGPT's vulnerabilities and the presentation of defense techniques.

In [38] ( β , ζ ), the authors focus on using BERT for cyber threat detection in IoT networks. Specifically, a new model security based upon the BERT architecture is presented for detecting cyber threats, with the usage of a novel privacy-preserving encoding technique. It should be borne in mind that technically speaking, BERT is a Small Language Model (SLM); however, the methods developed and employed are scalable to LLMs. The model is demonstrated to have high performance in threat detection.

In [39] ( α , β , γ ), the authors discuss how LLMs, despite their numerous advantageous applications, can be employed by cybercriminals for creating destructive payloads and tools. Further, the research methodically generated implementable code for the top-10 MITRE Techniques prevalent in 2022 using ChatGPT and conducted a comparative analysis of its performance with Gemini (formerly known as Bard).

In [40] (*), the authors discuss the general misuse of LLMs for various applications, not specifically for malware. However, it identifies malware generation as a potential ruinous use case. This paper serves as a primer for the identification of the vulnerabilities of LLMs, as well as mitigating factors for the prevention of misuse.

In [41] ( ϵ , ζ ), the authors focus on using ChatGPT for detecting phishing sites on the internet. In particular, they propose a new system that uses a web crawler to gather information from websites, generating prompts for LLMs based on the crawled data and retrieving detection results from the responses generated by the LLMs.

In [42] ( α ), the author pens a short paper in which the privacy and security implications of LLMs are explored. In particular, the paper investigates the potential risks posed by LLMs being used by malicious actors and examines the behaviors and practices of developers, operators, and users of these models from privacy and security perspectives.

In [43] ( ϵ , ζ ), the researchers strive to provide cybersecurity researchers and teams with easy-to-use and congenital model interaction to assess and respond to threats effectively. Specifically, they introduce a new framework, built on ChatGTP, that combines the power of ML and LLMs with XAI frameworks to enhance interpretability. The model is benchmarked and tested for accuracy using the KDD99 dataset [44, 45] and the Certified Information Security Manager (CISM) examinations.

In [46] and [47] ( α , β , γ ), the authors explore the usage of LLMs in the context of phishing attacks. Overall, these research pieces contribute to the understanding of large-scale attacks and how ML can be used for use cases involving LLM-generated content like phishing emails.

In [48] ( α ), the author addresses concerns regarding the negative impact of advanced textual models, such as assisting attackers in the creation of malware. The goal of the research is to investigate whether current large textual models, represented by GPT-3, can be used for generating malware and, if so, to understand how attackers could leverage these models for such purposes. This research provides an appraisal

of coding strategies for malware generation and the evaluation of malware variants produced by GPT-3.

Fig. 2 illustrates a comprehensive Venn diagram classification of the literature on the usage of LLMs for malware detection. Each section of the diagram represents distinct aspects of research in this domain, encompassing various methodologies and applications. Notably, the diagram highlights the interconnected nature of these studies, with many papers contributing to multiple areas of knowledge. For instance, references such as [33], [36], and [42] showcase the dual-use risks of LLMs in generating both beneficial and harmful content. The clustering of references like [17], [19], and [20] underscores the importance of addressing security vulnerabilities and adversarial attacks on LLMs. Additionally, the overlaps emphasize the multidimensional challenges and innovative solutions proposed by researchers, such as the usage of LLMs for real-time code analysis and trend identification. This visual taxonomy not only organizes existing research but also identifies gaps for future exploration, particularly in the areas of LLM-based malware generation and sophisticated evasion techniques. Overall, the figure serves as a roadmap for security researchers, guiding them through the complexities and interdependencies within this evolving field.

While we have tried to be as exhaustive in our taxonomy and coverage, we do not claim to have a review of every paper in the field, but at least some of the contemporary and influential works. Other resources are contained therein for further investigation.

We infer from the Venn diagram in Fig. 2 that material on the pure weaponization of LLMs ( β ) and LLM-based distribution ( γ ) of malware is non-existent. This is expected as having such information in the public domain would be disastrous. Additionally, we observe that this leaves many research gaps in the security domain, such as the design of algorithms and procedures that simultaneously use LLMs to generate and disseminate malware ( α ∩ γ ), creating harmful content using LLMs and then weaponizing it ( α ∩ β ), and the design of new models to weaponize LLMs ( β ∩ ζ ). Further research is required in these domains, with a specific emphasis on mitigating the risks associated with turning LLMs into weapons of militarization and cyberwarfare - Weapons of large destruction. Additionally, we observe that a gap in the literature exists in the examination of LLM-specific malware generation and distribution; most of the literature is centered around ChatGPT or Gemini - This presents an opportunity to examine how other LLMs, mentioned in Tab. 1, fair.

## 3 The Potential Use of LLMs for Protecting Against Malware - Problem Formulation and Performance Metrics

In this section, we describe various malware attack types and discuss how LLMs can be used as a contrivance in their prevention and spread. The caveat here is that while we acknowledge that malware is 'malicious software' created by the software itself, LLMs can be used to detect patterns in software code, for example, to flag a piece of code as potential malware.

There exists a profusion of methods and schemes to help fight against malware; for example, antivirus software is amongst the most ubiquitous. However, in the age

Fig. 3 Hierarchical classification of various methods through which LLMs can be employed to detect malware. The central node represents the core LLM technology, while the primary branches emanating from it illustrate specific techniques and applications. The secondary branches provide discussion points of their respective parent nodes.

<!-- image -->

of Generative AI (GenAI), LLMs can be used to gain a competitive advantage and stop a malware attack before or even while it is taking place. Specifically, LLMs can be used in the following manner:

1. Malware Honeypots: LLMs can generate realistically deceptive content, which includes phishing emails, fake websites, and documents. These can then be used as bait to lure attackers into honeypots. The interaction with these honeypots allows for the collection of data that can be used to enhance LLM training, and thereby improve threat detection capabilities. The engagement of the attackers with the malware honeypots can be calculated by simply counting all the interactions of each attacker with each of the malware honeypots. Mathematically,

<!-- formula-not-decoded -->

where I i,j is the number of interactions of attacker i with honeypot j .

2. Identification of Text-based Threats: LLMs can be trained to identify malicious code embedded within seemingly benign text files. For example, malware might use obfuscated code within a text-based file to bypass traditional security measures. The effectiveness of the LLM in detecting text-based threats can be evaluated as follows: Firstly, given a text-based file, determine the probability of a threat by building a binary classification model, and thereafter perform a characterization

according

<!-- formula-not-decoded -->

where threat = { 0 , 1 } is a binary variable with 0 indicating a non-threat, and 1 indicating a threat, w , b are the model's weights and bias term, σ is the sigmoid function, and x are the model features. Thereafter, all threats can simply be summed up to give a threat score based on the number of predicted matches. Mathematically,

<!-- formula-not-decoded -->

where p is a predicted to actual threat match.

3. Analysis of Code to Detect Nefarious Intent: LLMs can analyze the structure and semantics of code to detect potential malicious intent, even if the code is obfuscated or written in a non-standard way. The LLM can calculate a risk score by analyzing the code in its entirety or a partial snippet. Based on this risk score it can determine whether the code contains harmful objects. Mathematically, this risk score for a code c can be calculated as

<!-- formula-not-decoded -->

where T ( c ) is the abstract syntax tree, Γ : T → [0 , 1] such that it is a dictionary mapping from the abstract syntax tree to a numerical value that attempts to assign a weight to common keywords used in malicious code, and y ( n ) is the output from a classification model that maps the features to a risk score, analogous to Eqn. (7).

4. Malware Trend Identification: LLMs can process large datasets of past malware to identify trends and predict future attacks. This involves clustering similar malware samples and analyzing their evolution over time. One such way of identifying trends on a large corpus of malware data is metric-based clustering. For some latent features x i and x j representing the i th and j th instances respectively, the metric can be computed using cosine similarity, similar to NLP,

<!-- formula-not-decoded -->

where || · || 2 is the Euclidean distance. A distance d → 1 indicates a perfect match for malware, and d → -1 indicates that it is not malware. The time evolution component can be tracked by fitting some Markovian temporal model, say x i +1 = f ( x i , t | θ ) at time step t and parameters θ .

5. Non-standard Disguised Malware: As adversaries become more sophisticated in disguising malware, LLMs can be trained on adversarial examples to enhance detection capabilities. This involves generating adversarial samples and refining the model to improve its robustness. For example, GANs or VAEs can be used to take existing malware data, slightly perturb it, and then train the LLM to minimize the adversarial risk. Mathematically, given some malware sample x , perturb it with

some δ ≤ ϵ , where ϵ ≪ 1 is some small parameter, to create the new malware sample x ′ = x + δ . Then, train the LLM to solve the optimization problem

<!-- formula-not-decoded -->

where f is the predictor, J is the loss function, θ are the model parameters, and D is the training data. The solution to this problem exemplifies how an LLM can use existing malware training data to learn new threats via small perturbations.

In Fig. 3, we provide a pictorial representation of the various use cases for which LLMs can be used for protecting against malware, and in Fig. 4, we provide a detailed graphic of the various calculations of the metrics to demonstrate proof of concept.

Fig. 4 Graphical depiction of the mechanics of the proposed metric framework. The diagrams capture the five ways in which LLMs can be used for malware detection, involving simple counting in the case of malware honeypots and sophisticated optimization routines in the case of using existing malware code data to train for the detection of new threats.

<!-- image -->

## 4 Guiding Principles and Risk Mitigation Framework

## 4.1 Guiding Principles

Based upon the literature review above in Sec. 2, in this section, we propose the following guiding principles for using LLMs to detect malware. These are as follows:

1. Train the Model on a Large Corpora: The LLM should be trained on a vast amount of data from different sources so as to generalize to out-of-distribution use cases and realistically identify threats.
2. Identify Irregularities and Warning Signs: Train malware-specific LLMs on data that is composed of both malicious and congenial data that is both code (from different languages having different syntax) and text (documents, emails, etc.). The model should look out for linguistic traits that deviate from normalcy and, therefore, are indicative of maleficent intent.
3. Refine Threat Detection: Continuously improve threat detection capabilities by retraining models on a regular basis. Bearing in mind that LLMs have parameters in the order of 10 9 -10 12 , and are growing, retraining strategies like partial binarized training and compressed-weight training schemes.
4. Adopt a Human-centric Approach: The model should be trained, at least partially, using approaches like Reinforcement Learning from Human Feedback (RLHF) in order to align the intelligence of the LLM with that of human preferences in malware detection. Complete reliance and autonomy cannot be placed on either human beings or computers themselves. Thus, such approaches strike a balance in order to be effective in malware detection. Ultimately, a human expert should have the final say as to whether a threat is legitimate or not; the LLM should be used just as a detection tool.
5. Aggrandize Explainability: Interpretable AI techniques should be adopted in order to ascertain how LLM models arrived at their conclusion that a particular portion of the code is malware. This promotes trust in the LLM for being able to effectively detect malware threats and establish any biases the model may be inclined towards.
6. Performance Monitoring and Model Fine-tuning: Monitor the performance of LLM-based malware detection systems and adjust the training data and model parameters as needed in order to optimize the precision and accuracy and reduce the false positives. In addition, stay adept with the latest developments in LLM technologies, and incorporate these advancements into the existing models.
7. Integrate with Existing Security Infrastructure and Tools: The LLM malware detection system should be used concurrently with antivirus software, intrusion detection systems, and sandboxing.

The principles laid out address the concerns and lacuna identified in [20], and points 6 ○ and 7 ○ address the need for accommodative cybersecurity strategies, as identified in [22].

In Fig. 5, we provide a graphical summary of the various guiding principles. It is out belief that these principles cover the various use cases on how to leverage LLMs effectively for preventing risks associated with malware.

Based upon the guiding principles, we believe that they serve as a beacon for the compliance of organizations' cybersecurity departments to aim for and as a shepherd to guide the conformance of industry and governmental standards. In addition, we believe that these principles are modular, and can be tailored to a particular region, country, industry, or company in conjunction with other standards and regulations in the Age of Big Data such as the General Data Protection Regulation (GDPR) in the EU [49], the California Consumer Protection Act (CCPA) in the state of California, USA [50], or the Protection of Personal Information Act (POPIA) in South Africa [51].

Fig. 5 A graphical representation of the key guiding principles for effectively utilizing LLMs in malware detection. The diagram outlines a systematic approach, emphasizing the importance of data quality, model training, human oversight, and integration with existing security systems.

<!-- image -->

## 4.2 Risk Mitigation Strategies for Using LLMs to Prevent Malware

In this section, we propose a set of risk assuagement plans of action for detecting and preventing malware that is generated using LLMs:

1. Train the LLM Model on Confounded Data Samples: In order to make the LLM aware of different malware use cases like polymorphisms, encryption, and packing, the LLM should be trained on different samples in order to see through disguises. In addition, LLMs should focus not only on organic data but also on synthetic data from generative models in order to strengthen their ability to generalize.
2. Scan Code in Real-time: Of course, this is not possible with large LLMs; however, use lightweight LLMs to perform real-time scans on code for early detection of malware.
3. Generate Test Cases from Targeted Sandboxing: Generate targeted test cases that can be used to trigger the LLM's ability to flag malware in code, thereby allowing for a more comprehensive analysis and creating a dynamic feedback loop.
4. Heighten Security and Privacy through Federated Training: In order to increase the diversity of the training data and thereby its ability to detect differing malware types, consider training the model using federated learning and then generate a global model. This has the additional benefit of not training the model all on one machine, which makes it more agile.
5. Continuous Learning: Keep the LLM up-to-date with the latest developments in threats and security. This will ensure that it is effective in preventing new types of malware.

Point 5 ○ specifically addresses the need for novel approaches identified in [28]. In Fig. 6, we provide a graphical summary of the proposed risk mitigation strategies.

## 5 Performance Evaluation

In this section, we create conjectural scenarios that have fictitious data in order to demonstrate how the proposed metrics in Sec. 3 work. Specifically, we address each metric by providing situation-relevant data that could be collected in a real-world use case.

1. Malware Honeypots: Consider the scenario of one attacker that interacts 4 times with the first malware honeypot and 3 times with the second. These interactions could be phishing emails, fake account details, etc., designed to lure the attacker in. Then, the effectiveness is

<!-- formula-not-decoded -->

Metrics can then be tracked over time to form time series plots and be monitored.

Fig. 6 A graphical rendition of the proposed risk mitigation strategy for utilizing LLMs in malware detection. The diagram outlines a multi-faceted approach that encompasses data diversity, continuous learning, real-time code scanning, targeted sandboxing, and federated training.

<!-- image -->

2. Identification of Text-based Threats: Consider the situation where a textbased file had the keywords eval and exec . Given the simplistic model with parameters w = [0 . 30 , 0 . 72] T , b = 0, and features for this use case x = [1 . 30 , 2 . 25] T . Then, the probability it is a text-based threat equals

<!-- formula-not-decoded -->

So, it is reasonable to assume that there is a high chance that this is a text-based threat. Thereafter suppose that 100 such predictions resulted in these text files

- being flagged as threats, then the threat score is S = 100. Internal business rules can be decided upon as to what action needs to be taken.
3. Analysis of Code to Detect Nefarious Intent: Consider the abstract syntax tree comprising three nodes: T ( c ) = { n 1 -eval , n 2 = exec , n 3 = for } , and corresponding model outputs y ( n ) = { 0 . 82 , 0 . 94 , 0 . 12 } . Using the mapping

<!-- formula-not-decoded -->

the risk score is

<!-- formula-not-decoded -->

It is possible to conclude that values within this range result in code that has the potential to have malicious intent.

4. Malware Trend Identification: Consider the samples with latent features x 1 = [0 . 23 , 0 . 42] T and x 2 = [0 . 11 , 0 . 67] T , where x 1 are the features of a cluster in the training data, and x 2 are the latent features of potential malware whose trend we are trying to identify. Then, the metric is given by

<!-- formula-not-decoded -->

This value indicates a low potential for malware based on clustering.

5. Non-standard Disguised Malware: Consider the LLM performing the binary classification problem on a perturbed sample to classify it as malware or not malware. For this task, we use the binary cross-entropy loss function, the model f ( x ; θ ) = θ 1 x + θ 2 , and hyperparameters ϵ = 0 . 1 , δ = 0 . 02 , θ 1 = 0 . 5 , θ 2 = -1, and data in Tab. 4. In Tab. 4, we adjust the ˆ y values between the range [0 , 1] by

Table 4 Sample data for demonstrating Eqn. 10.

|   x |   y |   x ′ = x + δ | f = ˆ y = θ 1 x ′ + θ 2   | Adjusted ˆ y = clip ˆ y ( ε, 1 - ε )   |
|-----|-----|---------------|---------------------------|----------------------------------------|
|   1 |   0 |          1.02 | - 0 . 49                  | ε                                      |
|   2 |   1 |          2.02 | 0 . 01                    | 0 . 01                                 |
|   3 |   0 |          3.02 | 0 . 51                    | 0 . 51                                 |
|   4 |   1 |          4.02 | 1 . 01                    | 1 - ε                                  |
|   5 |   0 |          5.02 | 1 . 51                    | 1 - ε                                  |

clipping them to the range [ ε, 1 -ε ], with ε chosen to be 10 -15 . The loss function is then

<!-- formula-not-decoded -->

As the model training reduces this loss function, the LLM becomes better at identifying such disguised malware.

## 6 Conclusion

In this paper, a comprehensive literature review was presented, categorized, and discussed, in sufficient detail, on the use of LLMs to both generate malware and also to detect malware from code. The paper then provides comprehensive risk mitigation strategies and defines a set of performance factors that were used to evaluate the efficacy of the proposed strategies. In particular, the formulated metrics tackled five scenarios: Honeypots, text-based threats, code-based threats, identifying trends in existing malicious software, and using existing malicious software to create new code to train the LLM on. By creating conjectural situations with data, we were able to demonstrate the utility of these metrics. In addition, we suggested a set of general risk mitigation principles on the usage of LLMs for preventing malware and guiding principles on this matter.

However, while it holds immense potential, it should be noted that LLMs are not a silver bullet that can prevent or stop every genus of malware attack that exists. This is owing to the following three factors, which are graphically depicted in Fig. 7. First, the detection of false positives. As with any ML algorithm, the probability of detecting type 1 errors, where an affirmative for a malware attack when there is none exists. This is owing to the complexities of how hackers - who are human beings - write out and structure their code. Human beings have a tendency to be sloppy and use irrational variable assignments and sloppy code to bypass intrusion detection. Second, the ever-changing nature of malware. Malware does not remain static! Hackers are constantly changing around the code in order to make it detection-resistant. Thus, LLMs need to constantly be trained on new data to stay ahead of the curve and be resilient against such attacks. Finally, the explainability factor where LLMs have complex architectures, and therefore, it is very difficult to nearly impossible to explain the reasoning behind why a certain decision is made, for example, the flagging of a program as being malware. This is not only a problem with LLMs but with AI models in general; thus, this presents an argument for why we cannot be solely reliant on LLMs to definitively be our de facto malware detection systems.

Auxiliary future areas of exploration would be on leveraging LLMs for detecting malware on 4IR and 5IR devices, say on quantum computers quantum malware -whereby there exists potential exploitation of quantum mechanical properties such

Fig. 7 The key factors that can influence the effectiveness of LLMs in detecting and preventing various types of malware attacks. The checkmark ( ✓ ) in the green category and cross ( × ) in the red category within a Venn diagram symbolize the challenge of balancing detection accuracy with minimizing false positives, the upward-pointing arrows and a stylized bug icon represents the continuous evolution of malware threats, and the punctuation marks and a partially open box symbolize the inherent black-box nature of LLMs and the need for greater explainability.

<!-- image -->

as superposition and entanglement, or even the manipulation of the quantum data which exist as superimposed states, addressing the need for next-generation malware detection as highlighted in [29].

## Declarations

- Funding: J.A.K. and M.A.Z. acknowledge that this research is supported by grant number 23070, provided by Zayed University and the government of the UAE.
- Ethics approval and consent to participate: None required.
- Conflict of interest/Competing interests: The authors declare that there are no conflicts of interest.
- Consent for publication: The authors grant full consent to the journal to publish this article.

N/A

- Materials availability: N/A
- N/A
- Data availability:
- Code availability:
- Author contribution: All authors have contributed equally to this research.

## References

- [1] Roumeliotis, K.I., Tselikas, N.D.: Chatgpt and open-ai models: A preliminary review. Future Internet 15 (6), 192 (2023)
- [2] Wu, T., He, S., Liu, J., Sun, S., Liu, K., Han, Q.-L., Tang, Y.: A brief overview of chatgpt: The history, status quo and potential future development. IEEE/CAA

- Journal of Automatica Sinica 10 (5), 1122-1136 (2023)
- [3] Deng, J., Lin, Y.: The benefits and challenges of chatgpt: An overview. Frontiers in Computing and Intelligent Systems 2 (2), 81-83 (2022)
- [4] Katz, D.M., Bommarito, M.J., Gao, S., Arredondo, P.: Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society A 382 (2270), 20230254 (2024)
- [5] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)
- [6] Kung, J.E., Marshall, C., Gauthier, C., Gonzalez, T.A., Jackson III, J.B.: Evaluating chatgpt performance on the orthopaedic in-training examination. JBJS Open Access 8 (3), 23 (2023)
- [7] Gilson, A., Safranek, C.W., Huang, T., Socrates, V., Chi, L., Taylor, R.A., Chartash, D., et al. : How does chatgpt perform on the united states medical licensing examination (usmle)? the implications of large language models for medical education and knowledge assessment. JMIR Medical Education 9 (1), 45312 (2023)
- [8] Epstein, R.H., Dexter, F.: Variability in large language models' responses to medical licensing and certification examinations. comment on 'how does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment'. JMIR Medical Education 9 , 48305 (2023)
- [9] Terwiesch, C.: Would chat gpt3 get a wharton mba. A prediction based on its performance in the operations management course (2023)
- [10] Terwiesch, C.: Would chat gpt get a wharton mba. New white paper by Christian Terwiesch (2023)
- [11] Bhayana, R., Krishna, S., Bleakney, R.R.: Performance of chatgpt on a radiology board-style examination: insights into current strengths and limitations. Radiology 307 (5), 230582 (2023)
- [12] Newton, P.M., Xiromeriti, M.: Chatgpt performance on mcq exams in higher education. a pragmatic scoping review. EdArXiv. February 21 (2023)
- [13] Short, C.E., Short, J.C.: The artificially intelligent entrepreneur: Chatgpt, prompt engineering, and entrepreneurial rhetoric creation. Journal of Business Venturing Insights 19 , 00388 (2023)
- [14] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et al.: Siren's song in the ai ocean: a survey on hallucination in

- large language models. arXiv preprint arXiv:2309.01219 (2023)
- [15] Rawte, V., Sheth, A., Das, A.: A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922 (2023)
- [16] Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al.: A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232 (2023)
- [17] Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., Zhang, Y.: A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, 100211 (2024)
- [18] Beckerich, M., Plein, L., Coronado, S.: Ratgpt: Turning online llms into proxies for malware attacks. arXiv preprint arXiv:2308.09183 (2023)
- [19] Shayegani, E., Mamun, M.A.A., Fu, Y., Zaree, P., Dong, Y., Abu-Ghazaleh, N.: Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023)
- [20] Motlagh, F.N., Hajizadeh, M., Majd, M., Najafi, P., Cheng, F., Meinel, C.: Large language models in cybersecurity: State-of-the-art. arXiv preprint arXiv:2402.00891 (2024)
- [21] Xu, J., Stokes, J.W., McDonald, G., Bai, X., Marshall, D., Wang, S., Swaminathan, A., Li, Z.: Autoattacker: A large language model guided system to implement automatic cyber-attacks. arXiv preprint arXiv:2403.01038 (2024)
- [22] Vasconcelos, F.E., Almeida, G.S.: Llama assisted reverse engineering of modern ransomware: A comparative analysis with early crypto-ransomware (2023)
- [23] Agrawal, G., Kaur, A., Myneni, S.: A review of generative models in generating synthetic attack data for cybersecurity. Electronics 13 (2), 322 (2024)
- [24] Yamin, M.M., Hashmi, E., Ullah, M., Katt, B.: Applications of llms for generating cyber security exercise scenarios (2024)
- [25] Li, X., Zhu, T., Zhang, W.: Efficient ransomware detection via portable executable file image analysis by llama-7b (2023)
- [26] Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., Fritz, M.: Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In: Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pp. 79-90 (2023)
- [27] Liu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., Wang, H., Zheng, Y., Liu, Y.: Prompt injection attack against llm-integrated applications. arXiv preprint

- arXiv:2306.05499 (2023)
- [28] Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., Hashimoto, T.: Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733 (2023)
- [29] Madani, P.: Metamorphic malware evolution: The potential and peril of large language models. In: 2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA), pp. 74-81 (2023). IEEE Computer Society
- [30] Devadiga, D., Koo, H., Singh, A., Jin, G., Han, A., Chaudhari, K., Potdar, B., Shringi, A., Kumar, S.: Gleam: Gan and llm for evasive adversarial malware. In: 2023 14th International Conference on Information and Communication Technology Convergence (ICTC), pp. 53-58 (2023). IEEE
- [31] Glukhov, D., Shumailov, I., Gal, Y., Papernot, N., Papyan, V.: Llm censorship: A machine learning challenge or a computer security problem? arXiv preprint arXiv:2307.10719 (2023)
- [32] Fujima, H., Takeuchi, K., Kumamoto, T.: Semantic analysis of phishing emails leading to ransomware with chatgpt (2023)
- [33] Pa Pa, Y.M., Tanizaki, S., Kou, T., Van Eeten, M., Yoshioka, K., Matsumoto, T.: An attacker's dream? exploring the capabilities of chatgpt for developing malware. In: Proceedings of the 16th Cyber Security Experimentation and Test Workshop, pp. 10-18 (2023)
- [34] Alawida, M., Abu Shawar, B., Abiodun, O.I., Mehmood, A., Omolara, A.E., Al Hwaitat, A.K.: Unveiling the dark side of chatgpt: Exploring cyberattacks and enhancing user awareness. Information 15 (1), 27 (2024)
- [35] Begou, N., Vinoy, J., Duda, A., Korczy´ nski, M.: Exploring the dark side of ai: Advanced phishing attack design and deployment using chatgpt. In: 2023 IEEE Conference on Communications and Network Security (CNS), pp. 1-6 (2023). IEEE
- [36] Fang, R., Bindu, R., Gupta, A., Zhan, Q., Kang, D.: Llm agents can autonomously hack websites. arXiv preprint arXiv:2402.06664 (2024)
- [37] Gupta, M., Akiri, C., Aryal, K., Parker, E., Praharaj, L.: From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy. IEEE Access (2023)
- [38] Ferrag, M.A., Ndhlovu, M., Tihanyi, N., Cordeiro, L.C., Debbah, M., Lestable, T.: Revolutionizing cyber threat detection with large language models. arXiv preprint arXiv:2306.14263 (2023)

- [39] Charan, P., Chunduri, H., Anand, P.M., Shukla, S.K.: From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads. arXiv preprint arXiv:2305.15336 (2023)
- [40] Mozes, M., He, X., Kleinberg, B., Griffin, L.D.: Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833 (2023)
- [41] Koide, T., Fukushi, N., Nakano, H., Chiba, D.: Detecting phishing sites using chatgpt. arXiv preprint arXiv:2306.05816 (2023)
- [42] Kshetri, N.: Cybercrime and privacy threats of large language models. IT Professional 25 (3), 9-13 (2023)
- [43] Ali, T., Kostakos, P.: Huntgpt: Integrating machine learning-based anomaly detection and explainable ai with large language models (llms). arXiv preprint arXiv:2309.16021 (2023)
- [44] Olusola, A.A., Oladele, A.S., Abosede, D.O.: Analysis of kdd'99 intrusion detection dataset for selection of relevance features. In: Proceedings of the World Congress on Engineering and Computer Science, vol. 1, pp. 20-22 (2010). WCECS
- [45] ¨ Ozg¨ ur, A., Erdem, H.: A review of kdd99 dataset usage in intrusion detection and machine learning between 2010 and 2015 (2016)
- [46] Bethany, M., Galiopoulos, A., Bethany, E., Karkevandi, M.B., Vishwamitra, N., Najafirad, P.: Large language model lateral spear phishing: A comparative study in large-scale organizational settings. arXiv preprint arXiv:2401.09727 (2024)
- [47] Heiding, F., Schneier, B., Vishwanath, A., Bernstein, J., Park, P.S.: Devising and detecting phishing emails using large language models. IEEE Access (2024)
- [48] Botacin, M.: Gpthreats-3: Is automatic malware generation a threat? In: 2023 IEEE Security and Privacy Workshops (SPW), pp. 238-254 (2023). IEEE
- [49] Voigt, P., Bussche, A.: The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing 10 (3152676), 10-5555 (2017)
- [50] Pardau, S.L.: The california consumer privacy act: Towards a european-style privacy regime in the united states. J. Tech. L. &amp; Pol'y 23 , 68 (2018)
- [51] Adams, R., Adeleke, F., Anderson, D., Bawa, A., Branson, N., Christoffels, A., Vries, J.d., Etheredge, H., Flack-Davison, E., Gaffley, M., et al. : Popia code of conduct for research. South African Journal of Science 117 (5-6), 1-12 (2021)